import sys
import os
import re
import time
from datetime import datetime
import csv
import random
import logging
import pandas as pd
import traceback
from typing import List, Dict, Optional, Tuple

# Selenium Imports
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# --- CONFIG ---
port = 11434
OLLAMA_API_URL = f"http://localhost:{port}/api/generate"
OLLAMA_MODEL = "llama3.2"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# --- VERIFIER HANDLING ---
# ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° Import Verifier ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Mock Class ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÑ‡∏°‡πà Crash
try:
    from verifier import Verifier
except ImportError:
    logging.warning("‚ö†Ô∏è Could not import 'verifier.py'. Using DummyVerifier instead.")
    class Verifier:
        def verify(self, executives, html_content, bank_name):
            return {
                'is_complete': True,
                'missing_names': [],
                'extra_names': [],
                'error': None
            }

class FlexibleBankScraper:
    def __init__(self, base_url):
        self.base_url = base_url
        self.driver = None
        self.bank_name = None
        self.busi_dt = datetime.now().strftime("%Y-%m-%d")
        self.verified_executives = []


    def detect_bank_name(self, url: str, html_content: Optional[str] = None) -> str:
        url_lower = url.lower()
        logging.info(f"\n{'='*80}")
        logging.info(f"üîç BANK DETECTION DEBUG")
        logging.info(f"{'='*80}")
        logging.info(f"üîó URL: {url}")
        
        bank_keywords = {
            'bangkokbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'bbl': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'kasikorn': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'kbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'kasikornbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'scb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            'siamcommercial': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            'ktb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            'krungthai': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            'krungsri': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            'ttb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            'tmbthanachart': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            'kiatnakin': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£',
            'kkp': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£',
            'thanachart': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            'tisco': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏¥‡∏™‡πÇ‡∏Å‡πâ',
            'icbc': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏≠‡∏ã‡∏µ‡∏ö‡∏µ‡∏ã‡∏µ (‡πÑ‡∏ó‡∏¢)',
            'cimb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ã‡∏µ‡πÑ‡∏≠‡πÄ‡∏≠‡πá‡∏°‡∏ö‡∏µ ‡πÑ‡∏ó‡∏¢',
        }
        
        for keyword, bank_name in bank_keywords.items():
            if keyword in url_lower:
                logging.info(f" MATCH FOUND!!!")
                logging.info(f"    Keyword: '{keyword}'")
                logging.info(f"    Bank: {bank_name}")
                logging.info(f"{'='*80}\n")
                return bank_name
        
        logging.warning(f"‚ö†Ô∏è No bank detected from URL!")
        logging.warning(f"‚ö†Ô∏è Checking page content...\n")

        if html_content:
            soup = BeautifulSoup(html_content, 'html.parser')
            title = soup.find('title')
            if title:
                title_text = title.get_text().lower()
                for keyword, bank_name in bank_keywords.items():
                    if keyword in title_text:
                        logging.info(f" Bank detected from page title: {bank_name}")
                        return bank_name
            
            meta_tags = soup.find_all('meta', attrs={'name': ['description', 'title', 'og:title']})
            for meta in meta_tags:
                content = meta.get('content', '').lower()
                for keyword, bank_name in bank_keywords.items():
                    if keyword in content:
                        logging.info(f"üè¶ Bank detected from meta tags: {bank_name}")
                        return bank_name
            
            page_text = soup.get_text()
            
            thai_bank_exact = {
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
                '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
                'Bangkok Bank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
                '‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
                'Kasikornbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
                '‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
                '‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô',
                '‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô',
                'gsb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
                '‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            }
            
            for thai_keyword, bank_name in thai_bank_exact.items():
                if thai_keyword in page_text:
                    logging.info(f" Bank detected from the page text '{thai_keyword}': {bank_name}")
                    return bank_name
        
        domain_match = re.search(r'https?://(?:www\.)?([^/]+)', url)
        if domain_match:
            domain = domain_match.group(1)
            logging.warning(f" Could not auto-detect bank. Domain: {domain}")
            return f" ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ({domain})"
        
        return "‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"


    def setup_driver(self) -> bool:
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            ]
            chrome_options.add_argument(f"user-agent={random.choice(user_agents)}")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(60)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logging.info(" ---- WebDriver setup completed!!! ----- ")
            return True
            
        except Exception as e:
            logging.error(f"----- Error setting up WebDriver: {e} -----")
            return False


    def fetch_page_content(self, url: str, retries: int = 3) -> Optional[str]:
        if self.driver is None:
            if not self.setup_driver():
                return None
                
        for attempt in range(retries):
            try:
                time.sleep(random.uniform(2, 4))
                logging.info(f" Navigating to {url} (attempt {attempt+1}/{retries})")
                self.driver.get(url)
                
                WebDriverWait(self.driver, 20).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                logging.info(" ---- Waiting for content to load... ----")
                time.sleep(5)
                
                logging.info(" ---- Scrolling to load dynamic content... ----")
                for scroll_pct in [0.25, 0.5, 0.75, 1.0]:
                    self.driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {scroll_pct});")
                    time.sleep(2)
                
                self.driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(2)
                
                page_source = self.driver.page_source
                
                if len(page_source) > 500:
                    logging.info(f" ---- Page fetched successfully ({len(page_source)} chars) ---- ")
                    return page_source
                
            except Exception as e:
                logging.warning(f" ---- Error on attempt {attempt+1}: {e} ----")
                
            if attempt < retries - 1:
                time.sleep(5 * (attempt + 1))
        
        logging.error(" ---- Failed to fetch page after all retries ----")
        return None


    def _is_valid_thai_name(self, text: str, relaxed: bool = False) -> bool:
        if not text or len(text) < 8 or len(text) > 90:
            return False
        
        words = text.split()
        max_words = 8 if relaxed else 4
        if len(words) > max_words:
            return False
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏õ‡∏µ
        if re.search(r'\d{2,4}', text) and not re.search(r'(‡∏î‡∏£|‡∏®|‡∏£‡∏®|‡∏ú‡∏®)', text):
            return False
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå/‡∏≠‡∏µ‡πÄ‡∏°‡∏•
        if '@' in text or 'http' in text.lower() or '.com' in text.lower() or '.th' in text.lower():
            return False
        
        # Keywords ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏°‡∏≤‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
        conjoined_keywords = ['‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£', '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£', 'chief', 'officer', 'director', 'manager', '‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢', '‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô']
    
        invalid_keywords = [
            '‡∏™‡∏á‡∏ß‡∏ô‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', '‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', 'copyright', '¬©', 'all rights reserved',
            '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö‡∏°‡∏à', '‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡∏°‡∏´‡∏≤‡∏ä‡∏ô', 'limited', 'public', 'company',
            '‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå', 'website', 'www.', 'http', '.com', '.th', '.co',
            '‡πÇ‡∏ó‡∏£', '‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå', 'telephone', 'tel:', 'email', 'e-mail',
            '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠', 'contact', '‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°', 'information', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', 
            '‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà', 'address', '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà', 'location', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà',
            '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'date', '‡πÄ‡∏ß‡∏•‡∏≤', 'time', '‡∏õ‡∏µ', 'year', '‡∏û.‡∏®.', '‡∏Ñ.‡∏®.', 
            '‡πÄ‡∏°‡∏ô‡∏π', 'menu', '‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å', 'home', '‡∏Å‡∏•‡∏±‡∏ö', 'back', '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤', 'search', 
            '‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î', 'download', 'pdf', 'print', '‡∏û‡∏¥‡∏°‡∏û‡πå', '‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°', 'more', 
            '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', 'announcement', '‡∏Ç‡πà‡∏≤‡∏ß', 'news', '‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢', 'policy', '‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç', 
            '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß', 'privacy', '‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ', 'cookie', '‡∏£‡∏≤‡∏¢‡∏ô‡∏≤‡∏°', '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠', 
            '‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏´‡∏∏‡πâ‡∏ô', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', 'office'
        ]
        
        text_lower = text.lower()
        if any(keyword.lower() in text_lower for keyword in invalid_keywords):
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        if any(keyword in text for keyword in conjoined_keywords):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•) ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            last_word = words[-1]
            if any(last_word.endswith(keyword) for keyword in conjoined_keywords):
                return False

        special_char_count = sum(1 for char in text if char in '¬©¬Æ‚Ñ¢@#$%^&*()_+=[]{}|\\:;"<>,.?/')
        if special_char_count > 2:
            return False
        
        thai_char_count = sum(1 for char in text if 0x0E00 <= ord(char) <= 0x0E7F)
        min_thai_chars = 4 if relaxed else 7
        if thai_char_count < min_thai_chars:
            return False
        
        thai_titles = ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.', '‡∏î‡∏£', '‡∏®.', '‡∏£‡∏®.', '‡∏ú‡∏®.', 
                       '‡∏û‡∏•‡πÄ‡∏≠‡∏Å', '‡∏û‡∏•‡πÇ‡∏ó', '‡∏û‡∏•‡∏ï‡∏£‡∏µ', '‡∏û‡∏±‡∏ô‡πÄ‡∏≠‡∏Å', '‡∏û‡∏±‡∏ô‡πÇ‡∏ó', '‡∏û‡∏±‡∏ô‡∏ï‡∏£‡∏µ', 
                       '‡∏ó‡πà‡∏≤‡∏ô‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á', '‡∏Ñ‡∏∏‡∏ì‡∏´‡∏ç‡∏¥‡∏á', '‡∏Ñ‡∏∏‡∏ì', '‡∏û‡∏•.‡∏≠.‡∏≠.', '‡∏û.‡∏ï.‡∏≠.']
        
        if not any(text.startswith(title) for title in thai_titles):
            return False
        
        # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠/‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• 1 ‡∏Ñ‡∏≥ ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
        if len(words) < 2 and not any(title[:-1] in text for title in ['‡∏î‡∏£.', '‡∏®.', '‡∏£‡∏®.', '‡∏ú‡∏®.', '‡∏Ñ‡∏∏‡∏ì']): 
            return False
        
        return True


    def _is_valid_position(self, text: str, relaxed: bool = False) -> bool:
        """ Check if text is a valid position title """
        if not text or len(text) < 4 or len(text) > 100:
            return False
        
        valid_keywords = [
            '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£', '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£', '‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£', 'Chief',
            '‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢', '‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤', '‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö',
            'CEO', 'CFO', 'CTO', 'COO', 'President', 'Vice',
            'Executive', 'Director', 'Manager', 'Officer', 'Group', 'Advisor',
            'Assistant', 'Deputy', 'Senior', 'Head', 'Business',
            '‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤', '‡πÄ‡∏•‡∏Ç‡∏≤‡∏ô‡∏∏‡∏Å‡∏≤‡∏£', '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏ù‡πà‡∏≤‡∏¢', '‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', 
            'Audit', 'Board', 'Commercial', 'Compliance', 'Control', 
            'Corporate', 'Credit', 'Finance', 'Financial', 'Investment', 
            'Legal', 'Marketing', 'Operation', 'Product', 'Relationship', 
            'Risk', 'Sales', 'Strategy', 'Technology', 'Treasury', 'Wealth', 
            'Regional', 'Retail', 'Wholesale', 'SME', 'Digital',
            '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö‡∏°‡∏à', '‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡∏°‡∏´‡∏≤‡∏ä‡∏ô' 
        ]
        
        text_lower = text.lower()
        
        invalid_keywords = [
            '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠', 'copyright', '‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå', '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç', 
            '‡∏õ‡∏µ', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡πÄ‡∏°‡∏ô‡∏π', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å', 'home',
            '‡∏ó‡πà‡∏≤‡∏ô‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á', '‡∏Ñ‡∏∏‡∏ì‡∏´‡∏ç‡∏¥‡∏á', '‡∏Ñ‡∏∏‡∏ì', '‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.', '‡∏®.'
        ]
        
        if any(keyword in text_lower for keyword in invalid_keywords):
            return False
            
        if any(text.startswith(title) for title in ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.']):
            return False
        
        return any(keyword.lower() in text_lower for keyword in valid_keywords)


    def extract_executives_from_html(self, html_content: str) -> List[Tuple[str, str]]:
        #‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        soup = BeautifulSoup(html_content, 'html.parser')
        executives = []
        
        logging.info("\n Extracting executives from HTML...")
        
        # ‡∏•‡∏ö Element ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô
        for script in soup(["script", "style", "noscript", "footer", "header"]):
            if script: script.decompose()
        
        for nav in soup.find_all('nav'):
            if nav: nav.decompose()

        all_text_elements = soup.find_all(['p', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'li', 'td', 'th', 'a'])
        
        # Pre-process All Text
        processed_texts = []
        for element in all_text_elements:
            # ‡πÉ‡∏ä‡πâ separator= " " ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ HTML ‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏ô‡∏•‡∏∞ Tag
            # ‡πÄ‡∏ä‡πà‡∏ô <span>‡∏ô‡∏≤‡∏¢</span><span>‡∏Å‡∏≤‡∏ô‡∏ï‡πå</span> ‡∏à‡∏∞‡πÑ‡∏î‡πâ "‡∏ô‡∏≤‡∏¢ ‡∏Å‡∏≤‡∏ô‡∏ï‡πå" ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "‡∏ô‡∏≤‡∏¢‡∏Å‡∏≤‡∏ô‡∏ï‡πå"
            text = element.get_text(" ", strip=True) 
            text = re.sub(r'\s+', ' ', text).strip()
            
            if 4 <= len(text) <= 300:
                processed_texts.append((text, element))
        
        logging.info(f" ---- Total text elements to process: {len(processed_texts)} ---- ")
        
        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå
        def add_executive(name, position, source_pass):
            if name and position and name not in [n for n, p in executives] and position != "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏":
                if len(name.split()) > 7: 
                    return
                if len(position.split()) < 2 and any(position.startswith(title) for title in ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.']):
                    return
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥ (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á+‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)
                clean_name_check = self._parse_name_components(name)[2:]
                if clean_name_check[0] and clean_name_check in [self._parse_name_components(n)[2:] for n, p in executives]:
                    return

                executives.append((name, position))
                logging.debug(f" ---- {source_pass}: {name} | {position} ----")

        # ===== PASS 1: Table Extraction (Strongest signal) =====
        logging.info("\n ---- PASS 1: Extracting from tables... ---- ")
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                cell_texts = [re.sub(r'\s+', ' ', cell.get_text(strip=True)) for cell in cells if cell.get_text(strip=True)]
                
                name_found = None
                position_found = None
                
                for text in cell_texts:
                    if self._is_valid_thai_name(text):
                        name_found = text
                    elif self._is_valid_position(text):
                        position_found = text
                
                if name_found and position_found:
                    add_executive(name_found, position_found, "Table")
                elif name_found:
                    for text in cell_texts:
                        if text != name_found and self._is_valid_position(text):
                            add_executive(name_found, text, "Table (Adj)")
                            break
        
        logging.info(f" ---- After PASS 1 (Tables): {len(executives)} executives found ----")
        
        # ===== PASS 2: Adjacent Text in Containers =====
        logging.info("\n ---- PASS 2: Extracting from adjacent elements (normal) ---- ")
        
        for i, (text, element) in enumerate(processed_texts):
            if self._is_valid_thai_name(text):
                name = text
                position = None
                
                for j in range(i + 1, min(i + 7, len(processed_texts))):
                    next_text = processed_texts[j][0]
                    if self._is_valid_position(next_text) and not self._is_valid_thai_name(next_text, relaxed=True):
                        position = next_text
                        break
                
                if position:
                    add_executive(name, position, "Adjacent")
        
        logging.info(f"üìä After PASS 2 (Adjacent): {len(executives)} executives found")
        
        # ===== PASS 3: Relaxed Mode =====
        logging.info("\n ---- PASS 3: Extracting with relaxed criteria... ---- ")
        
        for i, (text, element) in enumerate(processed_texts):
            if self._is_valid_thai_name(text, relaxed=True):
                name = text
                position = None
                
                for j in range(i + 1, min(i + 10, len(processed_texts))):
                    next_text = processed_texts[j][0]
                    if self._is_valid_position(next_text, relaxed=True) and not self._is_valid_thai_name(next_text, relaxed=True):
                        position = next_text
                        break
                
                if position:
                    add_executive(name, position, "Relaxed")
        
        logging.info(f"üìä After PASS 3 (Relaxed): {len(executives)} executives found")
        
        # ===== PASS 4: Pattern Matching =====
        logging.info("\n PASS 4: Pattern-based extraction...")
        
        full_text = soup.get_text()
        
        pattern1 = r'((?:‡∏ô‡∏≤‡∏¢|‡∏ô‡∏≤‡∏á|‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß|‡∏î‡∏£\.|‡∏Ñ‡∏∏‡∏ì)[^\n]{10,80})\s+([^\n]{10,100}(?:‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£|‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£|‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£|Director|Manager|CEO|CFO|CTO|COO|President)[^\n]{0,50})'
        matches1 = re.findall(pattern1, full_text, re.IGNORECASE)
        
        for name_candidate, pos_candidate in matches1:
            name_clean = re.sub(r'\s+', ' ', name_candidate.strip())
            pos_clean = re.sub(r'\s+', ' ', pos_candidate.strip())
            
            if self._is_valid_thai_name(name_clean, relaxed=True) and self._is_valid_position(pos_clean, relaxed=True):
                add_executive(name_clean, pos_clean, "Pattern1")
        
        logging.info(f" After PASS 4 (Patterns): {len(executives)} executives found")

        # ===== PASS 5: Conjoined Name/Position Splitting =====
        logging.info("\nüîÑ PASS 5: Conjoined Name/Position Splitting/Trimming...")

        executives_to_process = executives.copy()
        executives = [] # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ Logic ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥
        conjoined_keywords_regex = r'(‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£|‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£|‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£|‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£|‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢|‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤|CEO|CFO|CTO|COO|President|Director|Manager|Chief)'
        next_name_prefix_regex = r'(‡∏ô‡∏≤‡∏¢|‡∏ô‡∏≤‡∏á|‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß|‡∏î‡∏£\.|‡∏Ñ‡∏∏‡∏ì)(?:\s+)?\S{2,}'
        
        for name, position in executives_to_process:
            
            name_was_split = False

            # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
            match_in_name = re.search(conjoined_keywords_regex, name)
            if match_in_name:
                split_index = match_in_name.start(0)
                name_candidate = name[:split_index].strip()
                pos_candidate = name[split_index:].strip()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏°‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤)
                if self._is_valid_thai_name(name_candidate, relaxed=True):
                    logging.info(f"  ‚úÇÔ∏è SPLIT Name: '{name}' -> Name: '{name_candidate}' | Pos: '{pos_candidate}'")
                    add_executive(name_candidate, position, "Split Name")
                    name_was_split = True
            
            # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Position ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏ï‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà
            if not name_was_split:
                match_in_pos = re.search(next_name_prefix_regex, position)
                if match_in_pos:
                    split_index = match_in_pos.start(0)
                    pos_clean = position[:split_index].strip()
                    
                    # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏°‡∏≤‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if self._is_valid_position(pos_clean, relaxed=True) and len(pos_clean) > 5:
                        logging.info(f"  ‚úÇÔ∏è TRIM Pos: '{position}' -> Trimmed: '{pos_clean}' (Found next name: {match_in_pos.group(0)})")
                        add_executive(name, pos_clean, "Trim Pos")
                        continue

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏î‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            if (name, position) not in executives:
                add_executive(name, position, "Unmodified")


        logging.info(f" ---- After PASS 5 (Splitting/Trimming): {len(executives)} executives found ---- ")
        
        # Step 6: Final Filter and Clean up
        final_executives = []
        seen_names_tuple = set()
        
        for name, position in executives:
            # Final check: ‡πÉ‡∏ä‡πâ _parse_name_components ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏ó‡∏µ‡πà "‡∏™‡∏∞‡∏≠‡∏≤‡∏î"
            eng_p, f_name_full, thai_p, f_name, s_name = self._parse_name_components(name)
            
            clean_name_key = (f_name, s_name)
            if not f_name or clean_name_key in seen_names_tuple:
                continue

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢ relaxed mode
            if self._is_valid_thai_name(name, relaxed=True):
                final_executives.append((name, position))
                seen_names_tuple.add(clean_name_key) 

        logging.info(f"\n ---- Total executives found after all passes: {len(final_executives)} ---- \n")

        return final_executives

    def _parse_name_components(self, Full_Name: str) -> Tuple[str, str, str, str, str]:
         #Logic ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Hardcode ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏Ç‡∏≤‡∏î
        full_name = Full_Name.strip()
        
        title_map = {
            "‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß": "Ms.",
            "‡∏ô‡∏≤‡∏¢": "Mr.", 
            "‡∏ô‡∏≤‡∏á": "Mrs.", 
            "‡∏î‡∏£.": "Dr.", 
            "‡∏î‡∏£": "Dr.", 
            "‡∏®.": "Prof.", 
            "‡∏£‡∏®.": "Assoc. Prof.", 
            "‡∏ú‡∏®.": "Asst. Prof.", 
            "‡∏û‡∏•‡πÄ‡∏≠‡∏Å": "Gen.", 
            "‡∏û‡∏•‡πÇ‡∏ó": "Lt. Gen.", 
            "‡∏û‡∏•‡∏ï‡∏£‡∏µ": "Maj. Gen.", 
            "‡∏û‡∏±‡∏ô‡πÄ‡∏≠‡∏Å": "Col.", 
            "‡∏û‡∏±‡∏ô‡πÇ‡∏ó": "Lt. Col.", 
            "‡∏û‡∏±‡∏ô‡∏ï‡∏£‡∏µ": "Maj.",
            "‡∏ó‡πà‡∏≤‡∏ô‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á": "Khunying", 
            "‡∏Ñ‡∏∏‡∏ì‡∏´‡∏ç‡∏¥‡∏á": "Khunying", 
            "‡∏Ñ‡∏∏‡∏ì": "Ms./Mr."
        }
        
        eng_prefix = ""
        thai_prefix = ""
        name_without_prefix = full_name
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥
        sorted_titles = sorted(title_map.items(), key=lambda x: len(x[0]), reverse=True)
        
        for thai_title, eng_title in sorted_titles:
            if full_name.startswith(thai_title):
                eng_prefix = eng_title
                thai_prefix = thai_title
                name_without_prefix = full_name[len(thai_title):].strip()
                break
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        name_without_prefix = re.sub(r'\s+', ' ', name_without_prefix).strip()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        conjoined_keywords_regex = r'(‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£|‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£|‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£|‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£|‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢|‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤|CEO|CFO|CTO|COO|President|Director|Manager|Chief|‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô|‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤)'
        match = re.search(conjoined_keywords_regex, name_without_prefix, re.IGNORECASE)
        
        if match:
            split_index = match.start(0)
            name_clean = name_without_prefix[:split_index].strip()
            if len(name_clean.split()) >= 1 and len(name_without_prefix) - len(name_clean) > 5:
                name_without_prefix = name_clean
        
        # ‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
        parts = name_without_prefix.split()
        
        if len(parts) == 0:
            # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏¢ -> ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
            return eng_prefix, full_name, "", ""
        elif len(parts) == 1:
            # ‡∏°‡∏µ‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
            first_name = parts[0]
            surname = ""
            full_name_thai = f"{thai_prefix}{first_name}" if thai_prefix else first_name
        else:
            # ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
            first_name = parts[0]
            surname = " ".join(parts[1:]).strip()
            
            # --- ‡∏ï‡∏±‡∏î‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢ ---
            if re.search(r'[\u0E00-\u0E7F]', surname):
                match_eng = re.search(r'[a-zA-Z]', surname)
                if match_eng:
                    eng_index = match_eng.start()
                    surname_clean = surname[:eng_index].strip()
                    if surname_clean:
                        logging.debug(f"  ‚úÇÔ∏è Trimmed English suffix: '{surname}' -> '{surname_clean}'")
                        surname = surname_clean
            
            # --- ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏£‡∏≠‡∏á" ‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏¢‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ---
            if surname.endswith("‡∏£‡∏≠‡∏á"):
                surname = re.sub(r'\s*‡∏£‡∏≠‡∏á$', '', surname).strip()
                logging.debug(f"  ‚úÇÔ∏è Trimmed trailing 'Rong' from surname")
            
            # Full_Name = ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏ó‡∏¢ + ‡∏ä‡∏∑‡πà‡∏≠ + ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
            full_name_thai = f"{thai_prefix}{first_name} {surname}" if thai_prefix else f"{first_name} {surname}"
        
        return eng_prefix, full_name_thai, thai_prefix, first_name, surname
    

    def create_executive_records(self, executives: List[Tuple[str, str]]) -> List[Dict]:
        records = []
        seen_names = set()
        
        logging.info("\n Creating executive records...")
        
        for name, position in executives:
            
            eng_prefix, full_name, thai_prefix, first_name, surname = self._parse_name_components(name)
            
            clean_name_key = (first_name, surname)
            if clean_name_key in seen_names:
                continue
            
            if not first_name or (not surname and len(first_name.split()) < 2 and len(first_name) < 4):
                logging.warning(f"  ‚ö†Ô∏è Could not parse name/surname effectively: {name}")
                continue
            
            record = {
                "BUSI_DT": self.busi_dt,
                "Eng_Prefix": eng_prefix,
                "Full_Name": full_name,
                "Thai_Prefix": thai_prefix,
                "First_Name": first_name,
                "Surname": surname,
                "Bank_Name": self.bank_name,
                "Position": position,
                "Source_URL": self.base_url,
            }
            

            records.append(record)
            seen_names.add(clean_name_key)
            logging.info(f" ---- {eng_prefix} | {thai_prefix} | {first_name} {surname} | {position} ----")
        
        return self._sort_executive_records(records)


    def intelligent_scrape(self, limit: int = 150) -> List[Dict]:
        logging.info(" ---- Starting scraping process... ---- ")
        
        html_content = self.fetch_page_content(self.base_url)
        if not html_content:
            logging.error(" ---- Failed to fetch page content ----")
            return []
        
        self.bank_name = self.detect_bank_name(self.base_url, html_content)
        logging.info(f" ---- Bank: {self.bank_name} ----")
        logging.info(f" ---- Business Date: {self.busi_dt} ----")
        
        executives = self.extract_executives_from_html(html_content)
        
        if not executives:
            logging.error(" ---- No executives found ---- ")
            return []
        
        records = self.create_executive_records(executives)
        
        logging.info(f"\n ---- Total records created: {len(records)} ---- ")
        return records[:limit]

    def _create_record_from_llm_data(self, full_name: str, position: str, confidence: float) -> Optional[Dict]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á executive record ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà LLM ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö
        Args:
            full_name: ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡πÄ‡∏ä‡πà‡∏ô "‡∏ô‡∏≤‡∏¢‡∏™‡∏°‡∏ä‡∏≤‡∏¢ ‡πÉ‡∏à‡∏î‡∏µ"
            position: ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô "‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏ç‡πà"
            confidence: ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô 0.0-1.0
        Returns:
            Dict record ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ
        """
        try:
            # Parse ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö
            eng_prefix, full_name_parsed, thai_prefix, first_name, surname = self._parse_name_components(full_name)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            if not first_name or len(first_name) < 2:
                logger.warning(f" ----  Invalid first name: {full_name} ----")
                return None
            
            if not position or len(position) < 4:
                logger.warning(f"  ---- Invalid position: {position} ---- ")
                return None
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á record ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö create_executive_records()
            record = {
                "BUSI_DT": self.busi_dt,
                "Eng_Prefix": eng_prefix,
                "Full_Name": full_name_parsed,
                "Thai_Prefix": thai_prefix,
                "First_Name": first_name,
                "Surname": surname,
                "Bank_Name": self.bank_name,
                "Position": position,
                "Source_URL": self.base_url,
                "Recovery_Confidence": confidence
            }

            logger.info(f"  ---- Created record: {eng_prefix} | {thai_prefix} {first_name} {surname} | {position} ---- ")
            return record
            
        except Exception as e:
            logger.error(f"  ---- Error creating record from LLM data: {e} ---- ")
            traceback.print_exc()
            return None
    
    def _sort_executive_records(self, records: List[Dict]) -> List[Dict]:
        """
        ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö executive records ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        Args:
            records: List ‡∏Ç‡∏≠‡∏á executive records
        Returns:
            List ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß
        """
        def sort_key(record):
            position = record['Position'].lower()
            
            # 0. ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: ‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£ (Chairman)
            if ('‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô' in position or 'chairman' in position) and '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£' in position and '‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£' not in position:
                return 0
            
            # 1. ‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ / CEO / President
            if '‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô' in position and ('‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£' in position or 'ceo' in position):
                return 1
            if 'president' in position and 'vice' not in position:
                return 1
            
            # 2. ‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô / Vice President
            if '‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô' in position or 'vice president' in position:
                return 2
            
            # 3. Chief Officers (CFO, CTO, COO, etc.)
            if '‡∏£‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏ç‡πà' in position or 'chief' in position or 'cfo' in position or 'cto' in position or 'coo' in position:
                return 3
            
            # 4. ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏ç‡πà / Managing Director
            if '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏ç‡πà' in position or 'managing director' in position or 'md' in position:
                return 4
            
            # 5. ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á / Executive
            if '‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£' in position or 'executive' in position or 'head of' in position:
                return 5
            
            # 6. ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó / Director
            if '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£' in position or 'director' in position:
                return 6
            
            # 7. ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            return 7
        
        try:
            sorted_records = sorted(records, key=sort_key)
            logger.info(f"‚úì Sorted {len(sorted_records)} records by position hierarchy")
            return sorted_records
        except Exception as e:
            logger.error(f" ---- Error sorting records: {e} ---- ")
            return records  # ‡∏ñ‡πâ‡∏≤ sort ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
        
    def close(self):
        """Close WebDriver"""
        try:
            if self.driver:
                self.driver.quit()
                logging.info(" ----  WebDriver closed ---- ")
        except Exception as e:
            logging.error(f" ---- Error closing WebDriver: {e} ---- ")
            
    def check_scraped_data_against_source(self, scraped_records: List[Dict]) -> List[Dict]:
        logging.info("\n ---- Starting data validation check (Smart & Relaxed)... ---- ")
        
        if not scraped_records:
            return []
        
        html_content = self.fetch_page_content(self.base_url) 
        if not html_content:
            return scraped_records

        soup = BeautifulSoup(html_content, 'html.parser')
        page_text = soup.get_text(" ", strip=True) # ‡πÉ‡∏ä‡πâ separator ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Page Text ‡πÅ‡∏ö‡∏ö‡πÑ‡∏£‡πâ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
        page_text_nospace = re.sub(r'\s+', '', page_text)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ record
        verified_records = []
        for record in scraped_records:
            full_name = record['Full_Name']
            first_name = record['First_Name']
            surname = record['Surname']
            is_verified = False
            
            names_to_check = [
                full_name,                                      # 1. ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏î‡∏¥‡∏°‡πÜ
                f"{first_name} {surname}".strip(),              # 2. ‡∏ä‡∏∑‡πà‡∏≠+‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• (‡πÄ‡∏ú‡∏∑‡πà‡∏≠ full_name ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô)
                f"{record['Eng_Prefix']} {first_name} {surname}".strip() # 3. ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤(Eng)+‡∏ä‡∏∑‡πà‡∏≠
            ]
            
            for name_variant in names_to_check:
                if name_variant and len(name_variant) > 3 and name_variant in page_text:
                    is_verified = True
                    logging.debug(f" ---- Verified (Standard): {name_variant} ---- " )
                    break
            
            # Check 2: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ
            if not is_verified:
                full_name_nospace = re.sub(r'\s+', '', full_name)
                # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÜ
                clean_name_nospace = re.sub(r'^(‡∏ô‡∏≤‡∏¢|‡∏ô‡∏≤‡∏á|‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß|‡∏î‡∏£\.|‡∏®\.|‡∏£‡∏®\.|‡∏Ñ‡∏∏‡∏ì)', '', full_name_nospace)
                
                if len(clean_name_nospace) > 4 and clean_name_nospace in page_text_nospace:
                    is_verified = True
                    logging.debug(f" ---- Verified (No-Space Match): {full_name} ---- ")

            if is_verified:
                verified_records.append(record)
            else:
                logging.warning(f" ---- Failed to verify: {full_name} (Not found in source text) ---- ")
                
        logging.info(f" ---- Verified records: {len(verified_records)} / {len(scraped_records)} ---- ")
        return verified_records

def save_to_csv(data: List[Dict], bank_name: str, busi_dt: str) -> bool:
    if not data:
        logging.error(" ---- No data to save. ---- ")
        return False
    try:
        df = pd.DataFrame(data)
        
        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πä‡∏∞‡πÜ
        column_order_save = [
            'BUSI_DT', 'Eng_Prefix', 'Full_Name', 'Thai_Prefix', 
            'First_Name', 'Surname', 'Bank_Name', 'Position'
        ]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ñ‡∏£‡∏ö‡πÑ‡∏´‡∏° ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÜ ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏±‡∏ô Error
        for col in column_order_save:
            if col not in df.columns:
                df[col] = ""
        
        # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        bank_short = bank_name.replace('‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '').strip()
        bank_name_map = {
            '‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': 'Kbank', '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': 'Bangkok', '‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': 'SCB',
            '‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': 'Krungthai', '‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': 'Krungsri', '‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': 'GSB',
            '‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï': 'TTB', '‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£': 'KKP', '‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï': 'Thanachart',
            '‡∏ó‡∏¥‡∏™‡πÇ‡∏Å‡πâ': 'TISCO', '‡πÑ‡∏≠‡∏ã‡∏µ‡∏ö‡∏µ‡∏ã‡∏µ (‡πÑ‡∏ó‡∏¢)': 'ICBC', '‡∏ã‡∏µ‡πÑ‡∏≠‡πÄ‡∏≠‡πá‡∏°‡∏ö‡∏µ ‡πÑ‡∏ó‡∏¢': 'CIMB'
        }
        
        file_bank_name = bank_short
        for thai_name, eng_name in bank_name_map.items():
            if thai_name in bank_short:
                file_bank_name = eng_name
                break
        
        date_str_month = datetime.now().strftime("%Y%m") 
        filename = f"{file_bank_name}_{date_str_month}.csv"
        os.makedirs('output', exist_ok=True)
        output_path = os.path.join('output', filename)
        
        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô Footer (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å URL)
        source_url = data[0].get('Source_URL', 'URL ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏') if data else "URL ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
        
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô column_order_save
        footer_row = {col: "" for col in column_order_save}
        footer_row['BUSI_DT'] = 'Source_URL:'
        footer_row['Eng_Prefix'] = source_url # ‡πÉ‡∏™‡πà URL ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ Source_URL:
        
        df_footer = pd.DataFrame([footer_row])
        
        # 4. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data + Footer) ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        df_final = pd.concat([df[column_order_save], df_footer], ignore_index=True)
        
        # 5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        df_final.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)
        
        print("\n" + "="*120)
        print(f" ---- RESULTS FOR {bank_name} ---- ")
        print(f" ---- Saved to: {output_path} ---- ")
        print(f" ---- Total: {len(df)} Executives ---- ")
        print("="*120)
        
        return True
        
    except Exception as e:
        logging.error(f" ---- Error saving CSV: {e} ----")
        traceback.print_exc()
        return False


def main():
    """Main execution - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-URL ‡∏û‡∏£‡πâ‡∏≠‡∏° Auto-Recovery ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ (v6.0)"""
    print("="*120)
    print(" BANK EXECUTIVE SCRAPER with Auto-Recovery Missing Data ")
    print("="*120)
    print("="*120 + "\n")
    
    checker = Verifier() 
    urls = [
        "https://www.kasikornbank.com/th/about/Pages/executives.aspx",
        # "https://www.scbx.com/th/executive-scbx/about-board-of-directors/",
        # "https://www.krungsri.com/th/about-krungsri/about-us/organization-chart/board-of-directors"
    ]
    
    print(f" ---- URLs to scrape: {urls} ----- ")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    print()
    
    all_results = []
    all_executives_data = []
    
    for url in urls:
        print(f"\n{'='*120}")
        print(f" Processing: {url}")
        print(f"{'='*120}\n")
        
        scraper = None
        
        try:
            scraper = FlexibleBankScraper(url)
            
            print(f"üåê Target URL: {url}")
            print(f"üìÖ Date: {scraper.busi_dt}")
            
            html_content = scraper.fetch_page_content(url)
            if not html_content:
                print(f"\n ---- FAILED: Could not fetch HTML content for {url} ---- ")
                continue

            scraper.bank_name = scraper.detect_bank_name(url, html_content)
            print(f" ---- Initial bank detection: {scraper.bank_name}\n ---- ")
            
            executives = scraper.extract_executives_from_html(html_content)
            
            if executives:
                print(f"\n Final detected bank: {scraper.bank_name}")
                
                records = scraper.create_executive_records(executives)
                
                # [VERIFICATION STEP 1] Internal Content Check
                verified_executives = scraper.check_scraped_data_against_source(records)
                
                print(f"\n After internal verification: {len(verified_executives)} executives")
                
                # [VERIFICATION STEP 2] LLM Verification
                llm_result = checker.verify(verified_executives, html_content, scraper.bank_name)
                
                print("\n" + "="*80)
                print("LLM VERIFICATION RESULTS")
                print("="*80)
                
                # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                final_data = verified_executives.copy()
                recovery_attempted = False
                
                if llm_result.get('is_complete', False):
                    print(f" ---- VERIFICATION SUCCESS: Data is COMPLETE and correct! ---- ")
                    
                elif llm_result.get('error'):
                    print(f" ---- VERIFICATION FAILED (API Error): {llm_result['error']} ----")
                    print(f" ---- Proceeding with scraped data without LLM recovery ---- ")
                    
                else:
                    missing = llm_result.get('missing_names', [])
                    extra = llm_result.get('extra_names', [])
                    
                    # Auto-Recovery Missing Data
                    if missing:
                        print(f"\n  INCOMPLETE: Found {len(missing)} missing name(s)")
                        print("="*80)
                        
                        recovery_attempted = True
                        recovered_count = 0
                        
                        for missing_entry in missing:
                            if isinstance(missing_entry, dict):
                                full_name = missing_entry.get('full_name', '')
                                position = missing_entry.get('position', '')
                                confidence = missing_entry.get('confidence', 0.0)
                                
                                print(f"\n RECOVERING: {full_name}")
                                print(f"   Position: {position}")
                                print(f"   Confidence: {confidence:.2%}")
                                
                                # ‡∏™‡∏£‡πâ‡∏≤‡∏á record ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                                recovered_record = scraper._create_record_from_llm_data(
                                    full_name, 
                                    position, 
                                    confidence
                                )
                                
                                if recovered_record:
                                    is_duplicate = any(
                                        r['Full_Name'] == recovered_record['Full_Name'] 
                                        for r in final_data
                                    )
                                    
                                    if not is_duplicate:
                                        final_data.append(recovered_record)
                                        recovered_count += 1
                                        print(f" ---- RECOVERED and ADDED to dataset ---- ")
                                    else:
                                        print(f" ---- SKIPPED: Duplicate entry detected ----")
                                else:
                                    print(f" ---- FAILED: Could not parse name components ---- ")
                            else:
                                # Old format (string only)
                                print(f" ---- Missing name: {missing_entry} ----")
                        
                        print(f"\n{'='*80}")
                        print(f" Recovery Summary: {recovered_count}/{len(missing)} entries recovered")
                        print(f" Final dataset: {len(final_data)} executives")
                        print(f"{'='*80}")
                    
                    if extra:
                        print(f"\n FALSE POSITIVES: Found {len(extra)} extra name(s):")
                        for name in extra:
                            print(f"   - {name}")
                        print(f"\n Consider reviewing these entries manually")
                
                print("="*80)

                if final_data:
                    all_executives_data.extend(final_data)
                    
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                    final_data_sorted = scraper._sort_executive_records(final_data)
                    
                    if save_to_csv(final_data_sorted, scraper.bank_name, scraper.busi_dt):
                        status_msg = "COMPLETE" if llm_result.get('is_complete') else "RECOVERED" if recovery_attempted else "INCOMPLETE"
                        print(f"\n SUCCESS: Saved {len(final_data_sorted)} executives from {scraper.bank_name} (Status: {status_msg})")
                        
                        all_results.append({
                            'bank': scraper.bank_name,
                            'count': len(final_data_sorted),
                            'url': url,
                            'llm_status': status_msg,
                            'recovered': len(final_data_sorted) - len(verified_executives) if recovery_attempted else 0
                        })
                    else:
                        print(f"\n  WARNING: Data extracted but failed to save CSV")
                else:
                    print(f"\n FAILED: No executives passed verification for {url}")
            else:
                print(f"\n  FAILED: No executives found for {url}")
            
        except KeyboardInterrupt:
            print("\n $ Interrupted by user $ ")
            break
        except Exception as e:
            logging.error(f" ---- Error processing {url}: {e} ---- ")
            import traceback
            traceback.print_exc()
        finally:
            if scraper:
                try:
                    scraper.close()
                except:
                    pass
    
    print("\n" + "="*120)
    print(" SCRAPING SUMMARY")
    print("="*120)
    
    if all_results:
        print(f"\n Successfully processed {len(all_results)} bank(s):\n")
        for i, result in enumerate(all_results, 1):
            recovered_info = f" (+{result['recovered']} recovered)" if result.get('recovered', 0) > 0 else ""
            print(f"  {i}. {result['bank']}: {result['count']} executives{recovered_info}")
            print(f"      Status: {result['llm_status']}")
            print(f"      URL: {result['url']}\n")
    else:
        print("\n No banks were successfully scraped")
    
    print("="*120)
    print("\n TIP: Check the 'output' folder for generated CSV files")
    print(" TIP: v6.0 automatically recovers missing executives with high confidence (>= 0.85)")
    print("="*120 + "\n")


if __name__ == "__main__":
    main()
