import pandas as pd
import logging
import sys
import os
import re
import time
import requests
import json
from datetime import datetime
import csv
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
from typing import List, Dict, Optional


# Configure logging
port = 11434
OLLAMA_API_URL = f"http://localhost:{port}/api/generate"
OLLAMA_MODEL = "llama3.2"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)


class OllamaAI:
    """Class to interact with Ollama API for AI-powered analysis"""
    
    def __init__(self, base_url=OLLAMA_API_URL, model=OLLAMA_MODEL):
        self.base_url = base_url
        self.model = model
        
    def generate_response(self, prompt: str, system_prompt: str = "") -> str:
        """Generate response using Ollama API"""
        try:
            url = f"{self.base_url}"
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "system": system_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "max_tokens": 2000
                }
            }
            
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "")
            
        except Exception as e:
            logging.error(f"Error generating AI response: {e}")
            return ""
    
    def test_connection(self) -> bool:
        """Test if Ollama is running and accessible"""
        try:
            response = requests.get(f"{self.base_url.replace('/api/generate', '')}", timeout=10)
            return response.status_code == 200
        except:
            return False


class IntelligentKBankScraper:
    """AI-powered adaptive web scraper for Bank executives"""
    
    def __init__(self, base_url="https://www.kasikornbank.com/th/about/Pages/executives.aspx"):
        self.base_url = base_url
        self.driver = None
        self.ai = OllamaAI()
        self.bank_name = None
        self.busi_dt = datetime.now().strftime("%Y-%m-%d")
        
        if not self.ai.test_connection():
            logging.warning(f"Ollama not accessible at {OLLAMA_API_URL}")

    def detect_bank_name(self, url: str, html_content: str) -> str:
        """Use AI to detect bank name from URL and page content"""
        
        system_prompt = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå

‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢:
- ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢
- ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û
- ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå
- ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢
- ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤

‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"""

        soup = BeautifulSoup(html_content, 'html.parser')
        title = soup.find('title')
        title_text = title.get_text() if title else ""
        
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        meta_text = meta_desc.get('content', '') if meta_desc else ""
        
        headings = []
        for h in soup.find_all(['h1', 'h2', 'h3'])[:5]:
            headings.append(h.get_text(strip=True))
        
        prompt = f"""‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ:

URL: {url}
‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏ô‡πâ‡∏≤: {title_text}
‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: {meta_text}
‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢: {', '.join(headings)}

‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£? ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""

        response = self.ai.generate_response(prompt, system_prompt)
        bank_name = response.strip().strip('"').strip("'").strip()
        
        if '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£' in bank_name and len(bank_name) < 100:
            logging.info(f"AI detected bank: {bank_name}")
            return bank_name
        
        return self._fallback_detect_bank(url)
    
    def _fallback_detect_bank(self, url: str) -> str:
        """Fallback method to detect bank from URL"""
        url_lower = url.lower()
        
        bank_keywords = {
            'kasikorn': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'kbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'bangkokbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'bbl': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'scb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            'ktb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            'krungsri': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
        }
        
        for keyword, bank_name in bank_keywords.items():
            if keyword in url_lower:
                return bank_name
        
        return "‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"

    def setup_driver(self) -> bool:
        """Setup Selenium WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            ]
            chrome_options.add_argument(f"user-agent={random.choice(user_agents)}")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(60)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logging.info("WebDriver setup completed")
            return True
            
        except Exception as e:
            logging.error(f"Error setting up WebDriver: {e}")
            return False

    def fetch_page_content(self, url: str, retries: int = 3) -> Optional[str]:
        """Fetch page content with retry logic"""
        if self.driver is None:
            if not self.setup_driver():
                return None
                
        for attempt in range(retries):
            try:
                time.sleep(random.uniform(2, 4))
                logging.info(f"Navigating to {url} (attempt {attempt+1})")
                self.driver.get(url)
                time.sleep(random.uniform(3, 6))
                
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                page_source = self.driver.page_source
                if len(page_source) > 500:
                    logging.info(f"Successfully fetched page ({len(page_source)} chars)")
                    return page_source
                
            except Exception as e:
                logging.warning(f"Error on attempt {attempt+1}: {e}")
                
            if attempt < retries - 1:
                time.sleep(5 * (attempt + 1))
        
        return None

    def _normalize_thai_text(self, text: str) -> str:
        """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ encoding ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        if not text:
            return ""
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        text = re.sub(r'[^\u0E00-\u0E7Fa-zA-Z\s\.]', '', text)
        
        return text

    def extract_executives_advanced(self, html_content: str) -> List[Dict]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        executives = []
        processed_names = set()
        
        logging.info("üîç Starting executive extraction...")
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å img tags ‡πÅ‡∏•‡∏∞ parent elements
        all_images = soup.find_all('img')
        logging.info(f"üì∑ Method 1: Found {len(all_images)} images")
        
        for idx, img in enumerate(all_images):
            try:
                # ‡∏´‡∏≤ parent containers ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö
                parents_to_check = []
                
                current = img
                for level in range(5):  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö 5 ‡∏£‡∏∞‡∏î‡∏±‡∏ö
                    parent = current.find_parent()
                    if parent:
                        parents_to_check.append(parent)
                        current = parent
                    else:
                        break
                
                for parent in parents_to_check:
                    text_content = parent.get_text(separator='|', strip=True)
                    lines = [line.strip() for line in text_content.split('|') if line.strip()]
                    
                    # Debug: ‡πÅ‡∏™‡∏î‡∏á 3 ‡∏£‡∏π‡∏õ‡πÅ‡∏£‡∏Å
                    if idx < 3:
                        logging.info(f"  Image {idx+1} nearby text (first 5 lines): {lines[:5]}")
                    
                    # ‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                    for i, line in enumerate(lines):
                        line_normalized = self._normalize_thai_text(line)
                        
                        if self._is_valid_executive_name(line_normalized):
                            name = line_normalized
                            position = lines[i+1] if i+1 < len(lines) else ""
                            position = self._normalize_thai_text(position)
                            
                            if name not in processed_names:
                                processed_names.add(name)
                                exec_data = self._create_executive_record(name, position)
                                if exec_data:
                                    executives.append(exec_data)
                                    logging.info(f"‚úÖ Method 1: {name} - {position}")
                                break
                    
                    if len(executives) > idx:  # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö parent ‡∏≠‡∏∑‡πà‡∏ô
                        break
                        
            except Exception as e:
                logging.debug(f"Error in Method 1, image {idx}: {e}")
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å tables
        logging.info(f"\nüìã Method 2: Searching in tables")
        tables = soup.find_all('table')
        logging.info(f"  Found {len(tables)} tables")
        
        for table_idx, table in enumerate(tables):
            try:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        cell_texts = [self._normalize_thai_text(c.get_text(strip=True)) for c in cells]
                        
                        for i, text in enumerate(cell_texts):
                            if self._is_valid_executive_name(text):
                                name = text
                                position = cell_texts[i+1] if i+1 < len(cell_texts) else ""
                                
                                if name not in processed_names:
                                    processed_names.add(name)
                                    exec_data = self._create_executive_record(name, position)
                                    if exec_data:
                                        executives.append(exec_data)
                                        logging.info(f"‚úÖ Method 2: {name} - {position}")
                                break
            except Exception as e:
                logging.debug(f"Error in Method 2, table {table_idx}: {e}")
        
        # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (fallback)
        if len(executives) < 5:  # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 5 ‡∏Ñ‡∏ô ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏µ‡πâ‡πÄ‡∏™‡∏£‡∏¥‡∏°
            logging.info(f"\nüîé Method 3: Full page scan (found only {len(executives)} so far)")
            
            body = soup.find('body')
            if body:
                all_text = body.get_text(separator='\n', strip=True)
                lines = [self._normalize_thai_text(line) for line in all_text.split('\n') if line.strip()]
                
                # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
                valid_lines = [line for line in lines if 5 <= len(line) <= 150]
                
                logging.info(f"  Processing {len(valid_lines)} text lines")
                
                i = 0
                while i < len(valid_lines):
                    line = valid_lines[i]
                    
                    if self._is_valid_executive_name(line):
                        name = line
                        position = valid_lines[i+1] if i+1 < len(valid_lines) else ""
                        
                        if name not in processed_names:
                            processed_names.add(name)
                            exec_data = self._create_executive_record(name, position)
                            if exec_data:
                                executives.append(exec_data)
                                logging.info(f"‚úÖ Method 3: {name} - {position}")
                            i += 2
                            continue
                    
                    i += 1
        
        logging.info(f"\nüìä Total executives extracted: {len(executives)}")
        return executives
    
    def _is_valid_executive_name(self, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà - ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ú‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏ô"""
        
        if not text:
            return False
        
        # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        if len(text) < 5 or len(text) > 100:
            return False
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢
        if not re.search(r'[\u0E00-\u0E7F]', text):
            return False
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠ (‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
        thai_prefixes = ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.', '‡∏®.', '‡∏£‡∏®.', '‡∏ú‡∏®.']
        has_prefix = any(text.startswith(p) for p in thai_prefixes)
        
        if not has_prefix:
            return False
        
        # ‡∏Å‡∏£‡∏≠‡∏á spam keywords ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        spam_keywords = [
            '‡∏Ñ‡∏•‡∏¥‡∏Å', '‡∏™‡∏°‡∏±‡∏Ñ‡∏£', '‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô', 
            'K PLUS', 'K-PLUS', 'KPLUS',
            '‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î', 'Download',
            '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡πÄ‡∏£‡∏≤', 'Contact',
            '‡∏™‡∏≤‡∏Ç‡∏≤', 'Branch',
            '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤', 'Search'
        ]
        
        text_upper = text.upper()
        for spam in spam_keywords:
            if spam.upper() in text_upper:
                return False
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏Ñ‡∏≥ (‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ + ‡∏ä‡∏∑‡πà‡∏≠/‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•)
        words = text.split()
        if len(words) < 2:
            return False
        
        # ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç = ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà valid
        return True
    
    def _create_executive_record(self, name: str, position: str) -> Optional[Dict]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"""
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
        name = self._normalize_thai_text(name)
        position = self._normalize_thai_text(position)
        
        if not name:
            return None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ position ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠
        if position and len(position) > 3:
            position_keywords = ['‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£', '‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£', 
                                '‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£', '‡∏£‡∏≠‡∏á', '‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢', '‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤',
                                '‡∏õ‡∏£‡∏∞‡∏à‡∏≥', '‡∏Å‡∏•‡∏∏‡πà‡∏°', '‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', '‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô']
            
            has_position_keyword = any(keyword in position for keyword in position_keywords)
            
            if not has_position_keyword:
                # ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                position = ""
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠
        prefix = self._extract_prefix_manual(name)
        first_name, surname = self._parse_name_manual(name)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á record
        executive = {
            "BUSI_DT": self.busi_dt,
            "Prefixed_Name": prefix,
            "Full_Name": name,
            "First_Name": first_name,
            "Surname": surname,
            "Bank_Name": self.bank_name,
            "Position": position if position else "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
        }
        
        return executive
    
    def _extract_prefix_manual(self, full_name: str) -> str:
        """‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠"""
        titles = {
            "‡∏ô‡∏≤‡∏¢": "Mr",
            "‡∏ô‡∏≤‡∏á": "Mrs",
            "‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß": "Ms",
            "‡∏î‡∏£.": "Dr",
            "‡∏®.": "Prof",
            "‡∏£‡∏®.": "Assoc Prof",
            "‡∏ú‡∏®.": "Asst Prof"
        }
        
        for thai_title, eng_title in sorted(titles.items(), key=lambda x: len(x[0]), reverse=True):
            if full_name.startswith(thai_title):
                return eng_title
        
        return ""
    
    def _parse_name_manual(self, full_name: str) -> tuple:
        """‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©"""
        # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
        titles = ["‡∏ô‡∏≤‡∏¢", "‡∏ô‡∏≤‡∏á", "‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß", "‡∏î‡∏£.", "‡∏®.", "‡∏£‡∏®.", "‡∏ú‡∏®."]
        
        name = full_name
        for title in sorted(titles, key=len, reverse=True):
            if name.startswith(title):
                name = name[len(title):].strip()
                break
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥
        parts = [p for p in name.split() if len(p) > 0]
        
        if len(parts) == 0:
            return "", ""
        elif len(parts) == 1:
            # ‡∏°‡∏µ‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            return parts[0], ""
        elif len(parts) == 2:
            # ‡∏õ‡∏Å‡∏ï‡∏¥: ‡∏ä‡∏∑‡πà‡∏≠ + ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
            return parts[0], parts[1]
        else:
            # ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 2 ‡∏Ñ‡∏≥: ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠ ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
            return parts[0], " ".join(parts[1:])
    
    def _is_duplicate(self, executive: Dict, executives_list: List[Dict]) -> bool:
        """‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô"""
        current_name = executive['Full_Name'].strip()
        current_position = executive['Position'].strip()
        
        for existing in executives_list:
            existing_name = existing['Full_Name'].strip()
            existing_position = existing['Position'].strip()
            
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
            if existing_name == current_name:
                # ‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                if existing_position == current_position or not current_position or not existing_position:
                    return True
        
        return False

    def intelligent_scrape(self, limit: int = 100) -> List[Dict]:
        """Main scraping function - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß"""
        logging.info("üöÄ Starting intelligent scraping...")
        
        # Fetch page
        html_content = self.fetch_page_content(self.base_url)
        if not html_content:
            logging.error("Failed to fetch page")
            return []
        
        # Detect bank name
        self.bank_name = self.detect_bank_name(self.base_url, html_content)
        logging.info(f"üè¶ Bank: {self.bank_name}")
        logging.info(f"üìÖ Business Date: {self.busi_dt}")
        
        # Extract executives
        logging.info("\nüì∏ Extracting executives from page...")
        all_executives = self.extract_executives_advanced(html_content)
        
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (double-check)
        unique_executives = []
        seen_names = set()
        
        for exec_data in all_executives:
            name = exec_data['Full_Name']
            if name not in seen_names:
                seen_names.add(name)
                unique_executives.append(exec_data)
        
        logging.info(f"\nüìä Total unique executives: {len(unique_executives)}")
        
        return unique_executives[:limit]

    def close(self):
        """Close WebDriver"""
        try:
            if self.driver:
                self.driver.quit()
                logging.info("WebDriver closed")
        except Exception as e:
            logging.error(f"Error closing WebDriver: {e}")


def save_to_csv(data: List[Dict], bank_name: str, busi_dt: str) -> bool:
    """Save data to CSV - one file per bank per month"""
    if not data:
        logging.warning("No data to save")
        return False

    try:
        df = pd.DataFrame(data)
        
        column_order = ['BUSI_DT', 'Prefixed_Name', 'Full_Name', 
                       'First_Name', 'Surname', 'Bank_Name', 'Position']
        df = df[column_order]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏°‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô-‡∏õ‡∏µ
        bank_short = bank_name.replace('‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '').strip()
        year_month = busi_dt[:7].replace('-', '')  # 2025-10-07 -> 202510
        filename = f"{bank_short}_{year_month}.csv"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå output ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        os.makedirs('output', exist_ok=True)
        output_path = os.path.join('output', filename)
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        file_exists = os.path.exists(output_path)
        
        if file_exists:
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤
            existing_df = pd.read_csv(output_path, encoding='utf-8-sig')
            
            # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            
            # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            combined_df = combined_df.drop_duplicates(subset=['Full_Name'], keep='last')
            
            # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó BUSI_DT ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà scraping ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            combined_df['BUSI_DT'] = busi_dt
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠
            combined_df = combined_df.sort_values('Full_Name').reset_index(drop=True)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
            combined_df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)
            
            logging.info(f"‚úÖ Updated {output_path}")
            logging.info(f"   Previous: {len(existing_df)} records")
            logging.info(f"   New: {len(df)} records")
            logging.info(f"   Total: {len(combined_df)} unique records")
            
            print("\n" + "="*100)
            print(f"üìä Updated Data for {bank_name} ({year_month[:4]}-{year_month[4:]})")
            print(f"üìÖ Last Scraped: {busi_dt}")
            print(f"üìÅ File: {output_path}")
            print(f"üìà Total Records: {len(combined_df)}")
            print("="*100)
            print(combined_df.to_string(index=False))
            print("="*100)
            
        else:
            # ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏•‡∏¢
            df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)
            
            logging.info(f"‚úÖ Created new file: {output_path}")
            logging.info(f"   Records: {len(df)}")
            
            print("\n" + "="*100)
            print(f"üìä New Data File Created for {bank_name} ({year_month[:4]}-{year_month[4:]})")
            print(f"üìÖ Scraped Date: {busi_dt}")
            print(f"üìÅ File: {output_path}")
            print(f"üìà Total Records: {len(df)}")
            print("="*100)
            print(df.to_string(index=False))
            print("="*100)
        
        return True
        
    except Exception as e:
        logging.error(f"Error saving CSV: {e}")
        return False


def main():
    """Main execution"""
    print("ü§ñ AI-Powered Bank Executive Scraper (Fixed Version)")
    print("=" * 60)
    
    try:
        scraper = IntelligentKBankScraper()
        
        if not scraper.ai.test_connection():
            print("‚ö†Ô∏è  Ollama not accessible")
            print(f"   URL: {OLLAMA_API_URL}")
            print(f"   Model: {OLLAMA_MODEL}")
            print("   (Scraping will continue without AI validation)")
        else:
            print("‚úÖ Ollama connected")
        
        print(f"\nüåê Target: {scraper.base_url}")
        print(f"üìÖ Date: {scraper.busi_dt}")
        
        executives = scraper.intelligent_scrape(limit=100)
        
        if executives:
            print(f"\n‚úÖ Extracted {len(executives)} executives")
            
            if save_to_csv(executives, scraper.bank_name, scraper.busi_dt):
                year_month = scraper.busi_dt[:7].replace('-', '')
                bank_short = scraper.bank_name.replace('‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '').strip()
                print(f"üíæ Saved to output/{bank_short}_{year_month}.csv")
        else:
            print("‚ùå No data extracted")
            
    except KeyboardInterrupt:
        print("\n‚ÑπÔ∏è  Interrupted")
    except Exception as e:
        logging.error(f"Error: {e}", exc_info=True)
    finally:
        if 'scraper' in locals():
            scraper.close()
        print("\nüèÅ Done")


if __name__ == "__main__":
    main()