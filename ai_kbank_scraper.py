from cgitb import text
import difflib
from turtle import position
import pandas as pd
import logging
import sys
import os
import re
import time
from datetime import datetime
import csv
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Tuple
from verifier import Verifier 


port = 11434
OLLAMA_API_URL = f"http://localhost:{port}/api/generate"
OLLAMA_MODEL = "llama3.2"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)


class FlexibleBankScraper:
    """Universal web scraper for Thai bank executives - Works with multiple banks"""

    def __init__(self, base_url):
        self.base_url = base_url
        self.driver = None
        self.bank_name = None
        self.busi_dt = datetime.now().strftime("%Y-%m-%d")
        self.verified_executives = []


    def detect_bank_name(self, url: str, html_content: Optional[str] = None) -> str:
        url_lower = url.lower()
        
        logging.info(f"\n{'='*80}")
        logging.info(f"üîç BANK DETECTION DEBUG")
        logging.info(f"{'='*80}")
        logging.info(f"üîó URL: {url}")
        logging.info(f"üîó URL (lowercase): {url_lower}")
        
        bank_keywords = {
            'bangkokbank.com': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'bangkokbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'bbl.co.th': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'bbl': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'kasikornbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'kasikorn': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'kbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'scb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            'scb.co.th': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            'siamcommercial': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            'ktb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            'krungthai': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            'ktb.co.th': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            'krungsri': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            'krungsri.com': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            'baya': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            'ttb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            'tmbthanachart': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            'kiatnakin': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£',
            'kkp': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£',
            'thanachart': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            'tisco': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏¥‡∏™‡πÇ‡∏Å‡πâ',
            'icbc': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏≠‡∏ã‡∏µ‡∏ö‡∏µ‡∏ã‡∏µ (‡πÑ‡∏ó‡∏¢)',
            'cimb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ã‡∏µ‡πÑ‡∏≠‡πÄ‡∏≠‡πá‡∏°‡∏ö‡∏µ ‡πÑ‡∏ó‡∏¢',
        }
        
        for keyword, bank_name in bank_keywords.items():
            if keyword in url_lower:
                logging.info(f"‚úÖ MATCH FOUND!")
                logging.info(f"    Keyword: '{keyword}'")
                logging.info(f"    Bank: {bank_name}")
                logging.info(f"{'='*80}\n")
                return bank_name
        
        logging.warning(f"‚ö†Ô∏è No bank detected from URL!")
        logging.warning(f"‚ö†Ô∏è Checking page content...\n")

        if html_content:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            title = soup.find('title')
            if title:
                title_text = title.get_text().lower()
                for keyword, bank_name in bank_keywords.items():
                    if keyword in title_text:
                        logging.info(f"üè¶ Bank detected from page title: {bank_name}")
                        return bank_name
            
            meta_tags = soup.find_all('meta', attrs={'name': ['description', 'title', 'og:title']})
            for meta in meta_tags:
                content = meta.get('content', '').lower()
                for keyword, bank_name in bank_keywords.items():
                    if keyword in content:
                        logging.info(f"üè¶ Bank detected from meta tags: {bank_name}")
                        return bank_name
            
            page_text = soup.get_text()
            
            thai_bank_exact = {
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
                '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
                'Bangkok Bank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
                '‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
                'KASIKORNBANK': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
                '‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
                '‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô',
                '‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô',
                'gsb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô',
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
                '‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            }
            
            for thai_keyword, bank_name in thai_bank_exact.items():
                if thai_keyword in page_text:
                    logging.info(f"‚úÖ Bank detected from page Thai text '{thai_keyword}': {bank_name}")
                    return bank_name
        
        domain_match = re.search(r'https?://(?:www\.)?([^/]+)', url)
        if domain_match:
            domain = domain_match.group(1)
            logging.warning(f"‚ö†Ô∏è Could not auto-detect bank. Domain: {domain}")
            return f"‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ({domain})"
        
        return "‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"


    def setup_driver(self) -> bool:
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            ]
            chrome_options.add_argument(f"user-agent={random.choice(user_agents)}")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(60)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logging.info("‚úÖ WebDriver setup completed")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Error setting up WebDriver: {e}")
            return False


    def fetch_page_content(self, url: str, retries: int = 3) -> Optional[str]:
        if self.driver is None:
            if not self.setup_driver():
                return None
                
        for attempt in range(retries):
            try:
                time.sleep(random.uniform(2, 4))
                logging.info(f"üåê Navigating to {url} (attempt {attempt+1}/{retries})")
                self.driver.get(url)
                
                WebDriverWait(self.driver, 20).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                logging.info("‚è≥ Waiting for content to load...")
                time.sleep(5)
                
                logging.info("üìú Scrolling to load dynamic content...")
                for scroll_pct in [0.25, 0.5, 0.75, 1.0]:
                    self.driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {scroll_pct});")
                    time.sleep(2)
                
                self.driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(2)
                
                page_source = self.driver.page_source
                
                if len(page_source) > 500:
                    logging.info(f"‚úÖ Page fetched successfully ({len(page_source)} chars)")
                    return page_source
                
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Error on attempt {attempt+1}: {e}")
                
            if attempt < retries - 1:
                time.sleep(5 * (attempt + 1))
        
        logging.error("‚ùå Failed to fetch page after all retries")
        return None


    def _is_valid_thai_name(self, text: str, relaxed: bool = False) -> bool:
        """
        [‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô (v5.1 - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô)
        """
        if not text or len(text) < 6 or len(text) > 90:
            return False
        
        words = text.split()
        max_words = 6 if relaxed else 5
        if len(words) > max_words:
            return False
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏õ‡∏µ
        if re.search(r'\d{2,4}', text) and not re.search(r'(‡∏î‡∏£|‡∏®|‡∏£‡∏®|‡∏ú‡∏®)', text):
            return False
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå/‡∏≠‡∏µ‡πÄ‡∏°‡∏•
        if '@' in text or 'http' in text.lower() or '.com' in text.lower() or '.th' in text.lower():
            return False
        
        # Keywords ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏°‡∏≤‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
        conjoined_keywords = ['‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£', '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£', 'chief', 'officer', 'director', 'manager', '‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢', '‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô']
        
        # [‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á] ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠
        invalid_keywords = [
            '‡∏™‡∏á‡∏ß‡∏ô‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', '‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', 'copyright', '¬©', 'all rights reserved',
            '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö‡∏°‡∏à', '‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡∏°‡∏´‡∏≤‡∏ä‡∏ô', 'limited', 'public', 'company',
            '‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå', 'website', 'www.', 'http', '.com', '.th', '.co',
            '‡πÇ‡∏ó‡∏£', '‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå', 'telephone', 'tel:', 'email', 'e-mail',
            '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠', 'contact', '‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°', 'information', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', 
            '‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà', 'address', '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà', 'location', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà',
            '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'date', '‡πÄ‡∏ß‡∏•‡∏≤', 'time', '‡∏õ‡∏µ', 'year', '‡∏û.‡∏®.', '‡∏Ñ.‡∏®.', 
            '‡πÄ‡∏°‡∏ô‡∏π', 'menu', '‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å', 'home', '‡∏Å‡∏•‡∏±‡∏ö', 'back', '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤', 'search', 
            '‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î', 'download', 'pdf', 'print', '‡∏û‡∏¥‡∏°‡∏û‡πå', '‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°', 'more', 
            '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', 'announcement', '‡∏Ç‡πà‡∏≤‡∏ß', 'news', '‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢', 'policy', '‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç', 
            '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß', 'privacy', '‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ', 'cookie', '‡∏£‡∏≤‡∏¢‡∏ô‡∏≤‡∏°', '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠', 
            '‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏´‡∏∏‡πâ‡∏ô', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', 'office'
        ]
        
        text_lower = text.lower()
        if any(keyword.lower() in text_lower for keyword in invalid_keywords):
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        if any(keyword in text for keyword in conjoined_keywords):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•) ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            last_word = words[-1]
            if any(last_word.endswith(keyword) for keyword in conjoined_keywords):
                return False

        special_char_count = sum(1 for char in text if char in '¬©¬Æ‚Ñ¢@#$%^&*()_+=[]{}|\\:;"<>,.?/')
        if special_char_count > 2:
            return False
        
        thai_char_count = sum(1 for char in text if 0x0E00 <= ord(char) <= 0x0E7F)
        min_thai_chars = 4 if relaxed else 5
        if thai_char_count < min_thai_chars:
            return False
        
        thai_titles = ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.', '‡∏î‡∏£', '‡∏®.', '‡∏£‡∏®.', '‡∏ú‡∏®.', 
                       '‡∏û‡∏•‡πÄ‡∏≠‡∏Å', '‡∏û‡∏•‡πÇ‡∏ó', '‡∏û‡∏•‡∏ï‡∏£‡∏µ', '‡∏û‡∏±‡∏ô‡πÄ‡∏≠‡∏Å', '‡∏û‡∏±‡∏ô‡πÇ‡∏ó', '‡∏û‡∏±‡∏ô‡∏ï‡∏£‡∏µ', 
                       '‡∏ó‡πà‡∏≤‡∏ô‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á', '‡∏Ñ‡∏∏‡∏ì‡∏´‡∏ç‡∏¥‡∏á', '‡∏Ñ‡∏∏‡∏ì']
        
        if not any(text.startswith(title) for title in thai_titles):
            return False
        
        if len(words) < 2:
            return False
        
        return True


    def _is_valid_position(self, text: str, relaxed: bool = False) -> bool:
        """[‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á] Check if text is a valid position title - v5.1"""
        if not text or len(text) < 4 or len(text) > 200:
            return False
        
        valid_keywords = [
            '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£', '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£', '‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£', 'Chief',
            '‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢', '‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤', '‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö',
            'CEO', 'CFO', 'CTO', 'COO', 'President', 'Vice',
            'Executive', 'Director', 'Manager', 'Officer', 'Group', 'Advisor',
            'Assistant', 'Deputy', 'Senior', 'Head', 'Business',
            '‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤', '‡πÄ‡∏•‡∏Ç‡∏≤‡∏ô‡∏∏‡∏Å‡∏≤‡∏£', '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏ù‡πà‡∏≤‡∏¢', '‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', 
            'Audit', 'Board', 'Commercial', 'Compliance', 'Control', 
            'Corporate', 'Credit', 'Finance', 'Financial', 'Investment', 
            'Legal', 'Marketing', 'Operation', 'Product', 'Relationship', 
            'Risk', 'Sales', 'Strategy', 'Technology', 'Treasury', 'Wealth', 
            'Regional', 'Retail', 'Wholesale', 'SME', 'Digital',
            '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö‡∏°‡∏à', '‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡∏°‡∏´‡∏≤‡∏ä‡∏ô' 
        ]
        
        text_lower = text.lower()
        
        invalid_keywords = [
            '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠', 'copyright', '‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå', '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç', 
            '‡∏õ‡∏µ', '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô', '‡πÄ‡∏°‡∏ô‡∏π', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å', 'home',
            '‡∏ó‡πà‡∏≤‡∏ô‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á', '‡∏Ñ‡∏∏‡∏ì‡∏´‡∏ç‡∏¥‡∏á', '‡∏Ñ‡∏∏‡∏ì', '‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.', '‡∏®.'
        ]
        
        if any(keyword in text_lower for keyword in invalid_keywords):
            return False
            
        if any(text.startswith(title) for title in ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.']):
            return False
        
        return any(keyword.lower() in text_lower for keyword in valid_keywords)


    def extract_executives_from_html(self, html_content: str) -> List[Tuple[str, str]]:
        """
        [‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡∏ç‡πà] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ HTML (v5.1)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° PASS 5: Conjoined Name/Position Splitting/Trimming
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        executives = []
        
        logging.info("\nüîç Extracting executives from HTML...")
        
        # ‡∏•‡∏ö Element ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô
        for script in soup(["script", "style", "noscript", "footer", "header"]):
            if script: script.decompose()
        
        for nav in soup.find_all('nav'):
            if nav: nav.decompose()

        all_text_elements = soup.find_all(['p', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'li', 'td', 'th', 'a'])
        
        # Step 1: Pre-process All Text
        processed_texts = []
        for element in all_text_elements:
            text = element.get_text(strip=True)
            text = re.sub(r'\s+', ' ', text).strip()
            
            if 5 <= len(text) <= 300: 
                processed_texts.append((text, element))
        
        logging.info(f"üìã Total text elements to process: {len(processed_texts)}")
        
        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå
        def add_executive(name, position, source_pass):
            if name and position and name not in [n for n, p in executives] and position != "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏":
                if len(name.split()) > 7: 
                    return
                if len(position.split()) < 2 and any(position.startswith(title) for title in ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.']):
                    return
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥ (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á+‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)
                clean_name_check = self._parse_name_components(name)[2:]
                if clean_name_check[0] and clean_name_check in [self._parse_name_components(n)[2:] for n, p in executives]:
                    return

                executives.append((name, position))
                logging.debug(f"‚úÖ {source_pass}: {name} | {position}")

        # ===== PASS 1: Table Extraction (Strongest signal) =====
        logging.info("\nüîÑ PASS 1: Extracting from tables...")
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                cell_texts = [re.sub(r'\s+', ' ', cell.get_text(strip=True)) for cell in cells if cell.get_text(strip=True)]
                
                name_found = None
                position_found = None
                
                for text in cell_texts:
                    if self._is_valid_thai_name(text):
                        name_found = text
                    elif self._is_valid_position(text):
                        position_found = text
                
                if name_found and position_found:
                    add_executive(name_found, position_found, "Table")
                elif name_found:
                    for text in cell_texts:
                        if text != name_found and self._is_valid_position(text):
                            add_executive(name_found, text, "Table (Adj)")
                            break
        
        logging.info(f"üìä After PASS 1 (Tables): {len(executives)} executives found")
        
        # ===== PASS 2: Adjacent Text in Containers (Normal mode) =====
        logging.info("\nüîÑ PASS 2: Extracting from adjacent elements (normal)...")
        
        for i, (text, element) in enumerate(processed_texts):
            if self._is_valid_thai_name(text):
                name = text
                position = None
                
                for j in range(i + 1, min(i + 7, len(processed_texts))):
                    next_text = processed_texts[j][0]
                    if self._is_valid_position(next_text) and not self._is_valid_thai_name(next_text, relaxed=True):
                        position = next_text
                        break
                
                if position:
                    add_executive(name, position, "Adjacent")
        
        logging.info(f"üìä After PASS 2 (Adjacent): {len(executives)} executives found")
        
        # ===== PASS 3: Relaxed Mode - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô =====
        logging.info("\nüîÑ PASS 3: Extracting with relaxed criteria...")
        
        for i, (text, element) in enumerate(processed_texts):
            if self._is_valid_thai_name(text, relaxed=True):
                name = text
                position = None
                
                for j in range(i + 1, min(i + 10, len(processed_texts))):
                    next_text = processed_texts[j][0]
                    if self._is_valid_position(next_text, relaxed=True) and not self._is_valid_thai_name(next_text, relaxed=True):
                        position = next_text
                        break
                
                if position:
                    add_executive(name, position, "Relaxed")
        
        logging.info(f"üìä After PASS 3 (Relaxed): {len(executives)} executives found")
        
        # ===== PASS 4: Pattern Matching - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏° Pattern ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ =====
        logging.info("\nüîÑ PASS 4: Pattern-based extraction...")
        
        full_text = soup.get_text()
        
        pattern1 = r'((?:‡∏ô‡∏≤‡∏¢|‡∏ô‡∏≤‡∏á|‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß|‡∏î‡∏£\.|‡∏Ñ‡∏∏‡∏ì)[^\n]{10,80})\s+([^\n]{10,100}(?:‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£|‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£|‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£|Director|Manager|CEO|CFO|CTO|COO|President)[^\n]{0,50})'
        matches1 = re.findall(pattern1, full_text, re.IGNORECASE)
        
        for name_candidate, pos_candidate in matches1:
            name_clean = re.sub(r'\s+', ' ', name_candidate.strip())
            pos_clean = re.sub(r'\s+', ' ', pos_candidate.strip())
            
            if self._is_valid_thai_name(name_clean, relaxed=True) and self._is_valid_position(pos_clean, relaxed=True):
                add_executive(name_clean, pos_clean, "Pattern1")
        
        logging.info(f"üìä After PASS 4 (Patterns): {len(executives)} executives found")

        # ===== PASS 5: Conjoined Name/Position Splitting (v5.1 - Improved Logic) =====
        logging.info("\nüîÑ PASS 5: Conjoined Name/Position Splitting/Trimming...")

        executives_to_process = executives.copy()
        executives = [] # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ Logic ‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥
        conjoined_keywords_regex = r'(‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£|‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£|‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£|‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£|‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢|‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤|CEO|CFO|CTO|COO|President|Director|Manager|Chief)'
        next_name_prefix_regex = r'(‡∏ô‡∏≤‡∏¢|‡∏ô‡∏≤‡∏á|‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß|‡∏î‡∏£\.|‡∏Ñ‡∏∏‡∏ì)(?:\s+)?\S{2,}'
        
        for name, position in executives_to_process:
            
            name_was_split = False

            # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡πÄ‡∏ä‡πà‡∏ô '‡∏≠‡∏±‡∏á‡∏®‡∏∏‡∏™‡∏¥‡∏á‡∏´‡πå‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô')
            match_in_name = re.search(conjoined_keywords_regex, name)
            if match_in_name:
                split_index = match_in_name.start(0)
                name_candidate = name[:split_index].strip()
                pos_candidate = name[split_index:].strip()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏°‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤)
                if self._is_valid_thai_name(name_candidate, relaxed=True):
                    logging.info(f"  ‚úÇÔ∏è SPLIT Name: '{name}' -> Name: '{name_candidate}' | Pos: '{pos_candidate}'")
                    add_executive(name_candidate, position, "Split Name")
                    name_was_split = True
            
            # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Position ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏ï‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà (‡πÄ‡∏ä‡πà‡∏ô '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ô‡∏≤‡∏¢‡∏Å‡∏¥‡∏ï‡∏ï‡∏¥‡∏û‡∏á‡∏®‡πå')
            if not name_was_split:
                match_in_pos = re.search(next_name_prefix_regex, position)
                if match_in_pos:
                    split_index = match_in_pos.start(0)
                    pos_clean = position[:split_index].strip()
                    
                    # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏°‡∏≤ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    if self._is_valid_position(pos_clean, relaxed=True) and len(pos_clean) > 5:
                        logging.info(f"  ‚úÇÔ∏è TRIM Pos: '{position}' -> Trimmed: '{pos_clean}' (Found next name: {match_in_pos.group(0)})")
                        add_executive(name, pos_clean, "Trim Pos")
                        continue

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏î‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            if (name, position) not in executives:
                add_executive(name, position, "Unmodified")


        logging.info(f"üìä After PASS 5 (Splitting/Trimming): {len(executives)} executives found")
        
        # Step 6: Final Filter and Clean up (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°)
        final_executives = []
        seen_names_tuple = set()
        
        for name, position in executives:
            # üí° Final check: ‡πÉ‡∏ä‡πâ _parse_name_components ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏ó‡∏µ‡πà "‡∏™‡∏∞‡∏≠‡∏≤‡∏î"
            prefix, full_name, first_name, surname = self._parse_name_components(name)
            
            clean_name_key = (first_name, surname)
            if not first_name or clean_name_key in seen_names_tuple:
                continue

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢ relaxed mode
            if self._is_valid_thai_name(name, relaxed=True):
                final_executives.append((name, position))
                seen_names_tuple.add(clean_name_key) 

        logging.info(f"\nüìä Total executives found after all passes: {len(final_executives)}")

        return final_executives


    def _parse_name_components(self, full_name: str) -> Tuple[str, str, str, str]:
        """
        [‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á] Parse name into prefix, full name, first name, and surname (v5.1)
        - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
        """
        title_map = {
            "‡∏ô‡∏≤‡∏¢": "Mr.", "‡∏ô‡∏≤‡∏á": "Mrs.", "‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß": "Ms.", 
            "‡∏î‡∏£.": "Dr.", "‡∏î‡∏£": "Dr.", "‡∏®.": "Prof.", 
            "‡∏£‡∏®.": "Assoc. Prof.", "‡∏ú‡∏®.": "Asst. Prof.", 
            "‡∏û‡∏•‡πÄ‡∏≠‡∏Å": "Gen.", "‡∏û‡∏•‡πÇ‡∏ó": "Lt. Gen.", "‡∏û‡∏•‡∏ï‡∏£‡∏µ": "Maj. Gen.", 
            "‡∏û‡∏±‡∏ô‡πÄ‡∏≠‡∏Å": "Col.", "‡∏û‡∏±‡∏ô‡πÇ‡∏ó": "Lt. Col.", "‡∏û‡∏±‡∏ô‡∏ï‡∏£‡∏µ": "Maj.",
            "‡∏ó‡πà‡∏≤‡∏ô‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á": "Khunying", "‡∏Ñ‡∏∏‡∏ì‡∏´‡∏ç‡∏¥‡∏á": "Khunying", "‡∏Ñ‡∏∏‡∏ì": "Ms./Mr."
        }
        
        prefix = ""
        name_without_prefix = full_name
        
        # 1. ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠
        for thai_title, eng_title in sorted(title_map.items(), key=lambda x: len(x[0]), reverse=True):
            if full_name.startswith(thai_title):
                prefix = eng_title
                name_without_prefix = full_name[len(thai_title):].strip()
                break
        
        name_without_prefix = re.sub(r'\s+', ' ', name_without_prefix).strip()
        
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏•‡∏±‡∏Å)
        conjoined_keywords_regex = r'(‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£|‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£|‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£|‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£|‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô|‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢|‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤|CEO|CFO|CTO|COO|President|Director|Manager|Chief|‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô|‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤)'
        match = re.search(conjoined_keywords_regex, name_without_prefix, re.IGNORECASE)
        
        if match:
            split_index = match.start(0)
            # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠ (‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•)
            name_clean = name_without_prefix[:split_index].strip()

            if len(name_clean.split()) >= 1 and len(name_without_prefix) - len(name_clean) > 5:
                 logging.debug(f"  ‚úÇÔ∏è Split Conjoined Name (in Parse): Original='{name_without_prefix}' -> Name='{name_clean}'")
                 name_without_prefix = name_clean


        parts = name_without_prefix.split()
        
        if len(parts) == 0:
            return prefix, full_name, "", ""
        elif len(parts) == 1:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
            return prefix, full_name, parts[0], ""
        else:
            # ‡∏ä‡∏∑‡πà‡∏≠ + ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
            first_name = parts[0]
            surname = " ".join(parts[1:])
            
            return prefix, full_name, first_name, surname.strip()


    def create_executive_records(self, executives: List[Tuple[str, str]]) -> List[Dict]:
        """
        [‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á List ‡∏Ç‡∏≠‡∏á Dictionary (Records) 
        """
        records = []
        seen_names = set()
        
        logging.info("\nüìù Creating executive records...")
        
        for name, position in executives:
            
            prefix, full_name, first_name, surname = self._parse_name_components(name)
            
            # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥
            clean_name_key = (first_name, surname)
            if clean_name_key in seen_names:
                continue
            
            if not first_name or not surname:
                # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏°‡∏µ First name ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏´‡∏≤‡∏Å Surname ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ (‡πÄ‡∏ä‡πà‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
                if not first_name or len(first_name) < 2:
                    logging.warning(f"  ‚ö†Ô∏è Could not parse name/surname: {name}")
                    continue
            
            record = {
                "BUSI_DT": self.busi_dt,
                "Prefixed_Name": prefix,
                "Full_Name": full_name, # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
                "First_Name": first_name,
                "Surname": surname,
                "Bank_Name": self.bank_name,
                "Position": position,
                "Source_URL": self.base_url, # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏° Source URL
            }
            
            records.append(record)
            seen_names.add(clean_name_key)
            logging.info(f"  ‚úÖ {prefix} | {first_name} {surname} | {position}")
        
        # üöÄ [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Logic ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        def sort_key(record):
            position = record['Position'].lower()
            
            # 1. ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: ‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£ (Chairman)
            if ('‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô' in position or 'chairman' in position) and '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£' in position and '‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£' not in position:
                return 0 


            if '‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô' in position and ('‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£' in position or 'ceo' in position):
                return 1
            if 'president' in position and 'vice' not in position:
                return 1

             
            if '‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô' in position or 'vice president' in position:
                return 2
            
            if '‡∏£‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏ç‡πà' in position or 'chief' in position or 'cfo' in position or 'cto' in position or 'coo' in position:
                return 3
            
            # 4. ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡∏°‡∏≤: ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏ç‡πà/‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏ç‡πà/MD
            if '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏ç‡πà' in position or 'managing director' in position or 'md' in position:
                return 4
            
            # 6. ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡∏°‡∏≤: ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á (Executive Vice President / Head of Group)
            if '‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£' in position or 'executive' in position or 'head of' in position:
                return 5
        

            # 5. ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡∏°‡∏≤: ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/Director
            if '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£' in position or 'director' in position:
                return 6
            
                
            # 8. ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î: ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            return 7
        
        records.sort(key=sort_key)
        
        if records:
            logging.info("\nüìã First 3 records structure after sorting and cleaning:")
            for i, record in enumerate(records[:3]):
                logging.info(f"  {i+1}. Prefixed: '{record['Prefixed_Name']}' | First: '{record['First_Name']}' | Last: '{record['Surname']}' | Pos: {record['Position']}")
        
        return records


    def intelligent_scrape(self, limit: int = 150) -> List[Dict]:
        logging.info("üöÄ Starting scraping process...")
        
        html_content = self.fetch_page_content(self.base_url)
        if not html_content:
            logging.error("‚ùå Failed to fetch page content")
            return []
        
        self.bank_name = self.detect_bank_name(self.base_url, html_content)
        logging.info(f"üè¶ Bank: {self.bank_name}")
        logging.info(f"üìÖ Business Date: {self.busi_dt}")
        
        executives = self.extract_executives_from_html(html_content)
        
        if not executives:
            logging.error("‚ùå No executives found")
            return []
        
        records = self.create_executive_records(executives)
        
        logging.info(f"\nüìä Total records created: {len(records)}")
        return records[:limit]

    def close(self):
        """Close WebDriver"""
        try:
            if self.driver:
                self.driver.quit()
                logging.info("‚úÖ WebDriver closed")
        except Exception as e:
            logging.error(f"‚ö†Ô∏è Error closing WebDriver: {e}")
            
    def check_scraped_data_against_source(self, scraped_records: List[Dict]) -> List[Dict]:
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Scrape ‡∏°‡∏≤‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Source URL
        """
        logging.info("\nüïµÔ∏è Starting data validation check...")
        
        if not scraped_records:
            logging.warning("‚ö†Ô∏è No records to check.")
            return []
        
        # [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏ï‡πâ‡∏≠‡∏á fetch content ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å self.driver ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏õ‡∏¥‡∏î‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
        html_content = self.fetch_page_content(self.base_url) 
        if not html_content:
            logging.error("‚ùå Failed to fetch page content for checking.")
            return scraped_records

        soup = BeautifulSoup(html_content, 'html.parser')
        page_text = soup.get_text()
        
        verified_records = []
        for record in scraped_records:
            full_name = record['Full_Name']
            first_name = record['First_Name']
            surname = record['Surname']
            
            # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°
            if full_name in page_text:
                verified_records.append(record)
                logging.debug(f"  ‚úÖ Verified (Full Name): {full_name}")
                continue
            
            # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á + ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•)
            name_without_title = f"{first_name} {surname}".strip()
            if len(name_without_title.split()) >= 2 and name_without_title in page_text:
                verified_records.append(record)
                logging.debug(f"  ‚úÖ Verified (First+Surname): {full_name}")
                continue
            
            # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•)
            if len(surname.split()) >= 1 and surname in page_text:
                verified_records.append(record)
                logging.debug(f"  ‚úÖ Verified (Surname): {full_name}")
                continue
                
            logging.warning(f"  ‚ùå Failed to verify (Name not found): {full_name}")
                
        logging.info(f"üìä Verified records: {len(verified_records)} / {len(scraped_records)}")
        return verified_records


def save_to_csv(data: List[Dict], bank_name: str, busi_dt: str) -> bool:
    """
    [‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á] Save data to CSV with proper formatting
    """
    if not data:
        logging.warning("‚ö†Ô∏è No data to save")
        return False

    try:
        df = pd.DataFrame(data)
        
        column_order = ['BUSI_DT', 'Prefixed_Name', 'Full_Name', 
                        'First_Name', 'Surname', 'Bank_Name', 'Position',]
        
        for col in column_order:
            if col not in df.columns:
                df[col] = ""
        
        df_executives = df[column_order].copy()
        
        bank_short = bank_name.replace('‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '').strip()
        
        bank_name_map = {
            '‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': 'Kbank',
            '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': 'Bangkok',
            '‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': 'SCB',
            '‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': 'Krungthai',
            '‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': 'Krungsri',
            '‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': 'GSB',
            '‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï': 'TTB',
            '‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£': 'KKP',
            '‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï': 'Thanachart',
            '‡∏ó‡∏¥‡∏™‡πÇ‡∏Å‡πâ': 'TISCO',
            '‡πÑ‡∏≠‡∏ã‡∏µ‡∏ö‡∏µ‡∏ã‡∏µ (‡πÑ‡∏ó‡∏¢)': 'ICBC',
            '‡∏ã‡∏µ‡πÑ‡∏≠‡πÄ‡∏≠‡πá‡∏°‡∏ö‡∏µ ‡πÑ‡∏ó‡∏¢': 'CIMB'
        }
        
        file_bank_name = bank_short
        for thai_name, eng_name in bank_name_map.items():
            if thai_name in bank_short:
                file_bank_name = eng_name
                break
        
        date_str_month = datetime.now().strftime("%Y%m") 
        filename = f"{file_bank_name}_{date_str_month}.csv"
        
        os.makedirs('output', exist_ok=True)
        output_path = os.path.join('output', filename)
        
        # üí° [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] ‡∏î‡∏∂‡∏á Source_URL ‡∏à‡∏≤‡∏Å Record ‡πÅ‡∏£‡∏Å ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö Footer
        source_url = df['Source_URL'].iloc[0] if not df.empty and 'Source_URL' in df.columns else "URL ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
        
        footer_data = {
            'BUSI_DT': source_url,           
            'Prefixed_Name': 'Source_URL',   
            'Full_Name': '', 
            'First_Name': '', 
            'Surname': '', 
            'Bank_Name': '', 
            'Position': '',
            
        }
        df_footer = pd.DataFrame([footer_data], columns=column_order)
        
        df_final = pd.concat([df_executives, df_footer], ignore_index=True)
        
        if os.path.exists(output_path):
            logging.info(f"üìÑ Appending to existing file: {output_path}")
            df_final.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)
            logging.info(f"‚úÖ Existing file overwritten with latest data.")
        else:
            df_final.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)
        
        logging.info(f"\n‚úÖ File saved: {output_path}")
        logging.info(f"üìä Total records (including footer): {len(df_final)}")
        
        print("\n" + "="*120)
        print(f"üìä RESULTS FOR {bank_name}")
        print(f"üìÖ Date: {busi_dt}")
        print(f"üìÅ File: {output_path}")
        print(f"üìà Records (Executives): {len(df_executives)}")
        print("="*120)
        
        print("\nüìã SAMPLE DATA (first 5 records):")
        pd.set_option('display.unicode.east_asian_width', True)
        pd.set_option('display.max_colwidth', 30)

        
        print("="*120 + "\n")
        
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error saving CSV: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Main execution - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-URL ‡∏û‡∏£‡πâ‡∏≠‡∏° Verification (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Verifier)"""
    print("="*120)
    print("ü§ñ IMPROVED BANK EXECUTIVE SCRAPER v5.2 (Plus LLM Verification - FIX)")
    print("="*120)
    print("‚úÖ **FIXED:** Import and usage of Verifier class.")
    print("‚úÖ **ENHANCEMENT:** Automated LLM Verification using Verifier.")
    print("‚úÖ **FIXED:** Source URL added to the last row of CSV.")
    print("‚úÖ **ENHANCEMENT:** Improved executive position sorting hierarchy.") # üëà **‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡πÅ‡∏à‡πâ‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç**
    print("="*120 + "\n")
    

    # [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] Initializer ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Verifier() ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ñ‡∏•‡∏≤‡∏™‡∏ä‡∏∑‡πà‡∏≠ Verifier ‡∏°‡∏≤
    checker = Verifier() 
    urls = [
        "https://www.kasikornbank.com/th/about/Pages/executives.aspx",
    ]
    
    print("üìã URLs to scrape:")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    print()
    
    all_results = []
    all_executives_data = []
    
    for url in urls:
        print(f"\n{'='*120}")
        print(f"üåê Processing: {url}")
        print(f"{'='*120}\n")
        
        scraper = None
        
        try:
            scraper = FlexibleBankScraper(url)
            
            print(f"üåê Target URL: {url}")
            print(f"üìÖ Date: {scraper.busi_dt}")
            
            html_content = scraper.fetch_page_content(url) # ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö HTML Content ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            if not html_content:
                print(f"\n‚ùå FAILED: Could not fetch HTML content for {url}")
                continue

            scraper.bank_name = scraper.detect_bank_name(url, html_content)
            print(f"üè¶ Initial bank detection: {scraper.bank_name}\n")
            
            executives = scraper.extract_executives_from_html(html_content)
            
            if executives:
                print(f"\nüè¶ Final detected bank: {scraper.bank_name}")
                
                records = scraper.create_executive_records(executives)
                
                # [NEW VERIFICATION STEP] 1: Run Internal Content Check
                verified_executives = scraper.check_scraped_data_against_source(records)
                
                # [NEW VERIFICATION STEP] 2: Run LLM Verification
                llm_result = checker.verify(verified_executives, html_content, scraper.bank_name)
                
                print("\n" + "="*80)
                print("üß† LLM VERIFICATION RESULTS")
                print("="*80)
                
                if llm_result.get('is_complete', False):
                    print(f"‚úÖ VERIFICATION SUCCESS: Data is COMPLETE and correct!")
                    final_data = verified_executives
                elif llm_result.get('error'):
                    print(f"‚ö†Ô∏è VERIFICATION FAILED (API Error): {llm_result['error']}")
                    final_data = verified_executives
                else:
                    missing = llm_result.get('missing_names', [])
                    extra = llm_result.get('extra_names', [])
                    
                    if missing:
                        print(f"‚ùå INCOMPLETE: Found {len(missing)} missing name(s):")
                        for name in missing:
                            print(f"   - {name}")
                    if extra:
                        print(f"‚ö†Ô∏è FALSE POSITIVES: Found {len(extra)} extra name(s) in scraped data (remove these):")
                        for name in extra:
                            print(f"   - {name}")
                    
                    # ‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Scrape ‡∏°‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ
                    final_data = verified_executives 
                
                print("="*80)

                if final_data:
                    all_executives_data.extend(final_data) 
                    
                    if save_to_csv(final_data, scraper.bank_name, scraper.busi_dt):
                        print(f"\n‚úÖ SUCCESS: Extracted and Verified {len(final_data)} executives from {scraper.bank_name}")
                        all_results.append({
                            'bank': scraper.bank_name,
                            'count': len(final_data),
                            'url': url,
                            'llm_status': 'Complete' if llm_result.get('is_complete') else 'Incomplete'
                        })
                    else:
                        print(f"\n‚ö†Ô∏è WARNING: Data extracted but failed to save CSV")
                else:
                    print(f"\n‚ùå FAILED: No executives passed the verification for {url}")
            else:
                print(f"\n‚ùå FAILED: No executives found for {url}")
            
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è Interrupted by user")
            break
        except Exception as e:
            logging.error(f"‚ùå Error processing {url}: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if scraper:
                try:
                    scraper.close()
                except:
                    pass
    
    print("\n" + "="*120)
    print("üìã SCRAPING SUMMARY")
    print("="*120)
    
    if all_results:
        print(f"\n‚úÖ Successfully scraped {len(all_results)} bank(s):\n")
        for i, result in enumerate(all_results, 1):
            print(f"  {i}. {result['bank']}: {result['count']} executives")
            print(f"      URL: {result['url']}\n")
    else:
        print("\n‚ùå No banks were successfully scraped")
    
    print("="*120)
    print("\nüí° TIP: Check the 'output' folder for generated CSV files")
    print("üí° TIP: v5.2 now uses the LLM Verifier for final data quality check.")
    print("="*120 + "\n")


if __name__ == "__main__":
    main()