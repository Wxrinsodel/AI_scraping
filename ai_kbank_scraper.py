from cgitb import text
import difflib
from turtle import position
import pandas as pd
import logging # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
import sys
import os
import re
import time
import requests
import json
from datetime import datetime
import csv
import random
from selenium import webdriver #‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏ö‡∏ö‡πÑ‡∏î‡∏ô‡∏≤‡∏°‡∏¥‡∏Å
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Tuple


"""
‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OLLAMA API: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏≠‡∏£‡πå‡∏ï URL ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö API ‡∏Ç‡∏≠‡∏á OLLAMA 
‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô LLM ‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á)
"""
port = 11434
OLLAMA_API_URL = f"http://localhost:{port}/api/generate"
OLLAMA_MODEL = "llama3.2"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__) # 


class FlexibleBankScraper:
    """Universal web scraper for Thai bank executives - Works with multiple banks"""

    def __init__(self, base_url): #store as the name said
        self.base_url = base_url
        self.driver = None
        self.bank_name = None
        self.busi_dt = datetime.now().strftime("%Y-%m-%d")
        self.verified_executives = []


    def detect_bank_name(self, url: str, html_content: Optional[str] = None) -> str:
        #‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏à‡∏≤‡∏Å URL ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏´‡∏≤‡∏Å‡∏°‡∏µ
        url_lower = url.lower()
        
        logging.info(f"\n{'='*80}")
        logging.info(f"üîç BANK DETECTION DEBUG")
        logging.info(f"{'='*80}")
        logging.info(f"üîó URL: {url}")
        logging.info(f"üîó URL (lowercase): {url_lower}")
        
        # Extended bank keyword mapping
        bank_keywords = {
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û
            'bangkokbank.com': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'bangkokbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'bbl.co.th': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            'bbl': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢
            'kasikornbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'kasikorn': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            'kbank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå
            'scb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            'scb.co.th': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            'siamcommercial': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢
            'ktb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            'krungthai': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            'ktb.co.th': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤
            'krungsri': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            'krungsri.com': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            'baya': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï
            'ttb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            'tmbthanachart': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£
            'kiatnakin': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£',
            'kkp': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï
            'thanachart': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏¥‡∏™‡πÇ‡∏Å‡πâ
            'tisco': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏¥‡∏™‡πÇ‡∏Å‡πâ',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏≠‡∏ã‡∏µ‡∏ö‡∏µ‡∏ã‡∏µ (‡πÑ‡∏ó‡∏¢)
            'icbc': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏≠‡∏ã‡∏µ‡∏ö‡∏µ‡∏ã‡∏µ (‡πÑ‡∏ó‡∏¢)',
            
            # ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ã‡∏µ‡πÑ‡∏≠‡πÄ‡∏≠‡πá‡∏°‡∏ö‡∏µ ‡πÑ‡∏ó‡∏¢
            'cimb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ã‡∏µ‡πÑ‡∏≠‡πÄ‡∏≠‡πá‡∏°‡∏ö‡∏µ ‡πÑ‡∏ó‡∏¢',
        }
        
        # First > try ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å URL: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÉ‡∏ô Dictionary ‡∏´‡∏≤‡∏Å‡∏û‡∏ö Keyword‡πÉ‡∏ôURL‡∏à‡∏∞return‡∏ä‡∏∑‡πà‡∏≠‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        for keyword, bank_name in bank_keywords.items():
            if keyword in url_lower:
                logging.info(f"‚úÖ MATCH FOUND!")
                logging.info(f"   Keyword: '{keyword}'")
                logging.info(f"   Bank: {bank_name}")
                logging.info(f"{'='*80}\n")
                return bank_name
        
        logging.warning(f"‚ö†Ô∏è No bank detected from URL!")
        logging.warning(f"‚ö†Ô∏è Checking page content...\n")

        # Second > try ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ HTML: 
        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏à‡∏≤‡∏Å URL ‡∏à‡∏∞‡πÉ‡∏ä‡πâ BeautifulSoup ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Title Tag, Meta Tags, ‡πÅ‡∏•‡∏∞ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö

        if html_content:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Check title tag
            title = soup.find('title')
            if title:
                title_text = title.get_text().lower()
                for keyword, bank_name in bank_keywords.items():
                    if keyword in title_text:
                        logging.info(f"üè¶ Bank detected from page title: {bank_name}")
                        return bank_name
            
            # Check meta tags
            meta_tags = soup.find_all('meta', attrs={'name': ['description', 'title', 'og:title']})
            for meta in meta_tags:
                content = meta.get('content', '').lower()
                for keyword, bank_name in bank_keywords.items():
                    if keyword in content:
                        logging.info(f"üè¶ Bank detected from meta tags: {bank_name}")
                        return bank_name
            
            # Check for Thai bank names in page content
            page_text = soup.get_text()
            
            thai_bank_exact = {
                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
                '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û',
                'Bangkok Bank': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û', # blocked

                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
                '‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢',
                'KASIKORNBANK': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢', # test already

                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå',
                '‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå', # test already

                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢',
                '‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢', # blocked

                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô',
                '‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô',
                'gsb': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô', # test already

                '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤',
                '‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤', # test already

            }
            
            for thai_keyword, bank_name in thai_bank_exact.items():
                if thai_keyword in page_text:
                    logging.info(f"‚úÖ Bank detected from page Thai text '{thai_keyword}': {bank_name}")
                    return bank_name
        
        # If still not detected, try to extract from domain
        domain_match = re.search(r'https?://(?:www\.)?([^/]+)', url)
        if domain_match:
            domain = domain_match.group(1)
            logging.warning(f"‚ö†Ô∏è Could not auto-detect bank. Domain: {domain}")
            return f"‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ ({domain})"
        
        return "‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"


    def setup_driver(self) -> bool:

        """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Selenium WebDriver (Chrome)
        ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ChromeOptions ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Headless (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå) 
        ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Bot 
        (‡πÄ‡∏ä‡πà‡∏ô user-agent, excludeSwitches, useAutomationExtension)
        """

        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            ]
            chrome_options.add_argument(f"user-agent={random.choice(user_agents)}")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(60) # 60 seconds timeout

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ JavaScript ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ã‡πà‡∏≠‡∏ô Property navigator.webdriver ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏µ‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Selenium
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logging.info("‚úÖ WebDriver setup completed")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Error setting up WebDriver: {e}")
            return False


    def fetch_page_content(self, url: str, retries: int = 3) -> Optional[str]:
        """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Selenium"""
        if self.driver is None:
            if not self.setup_driver():
                return None
                
        for attempt in range(retries):
            try:
                time.sleep(random.uniform(2, 4)) # wait before each attempt to avoid detection
                logging.info(f"üåê Navigating to {url} (attempt {attempt+1}/{retries})")
                self.driver.get(url)
                
                # ‡∏£‡∏≠‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ Element <body/> ‡∏à‡∏∞‡∏õ‡∏£‡∏≤‡∏Å‡∏è (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 20 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ) 
                # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß
                WebDriverWait(self.driver, 20).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                logging.info("‚è≥ Waiting for content to load...")
                time.sleep(5)
                
                # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå Scroll ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö 
                # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Lazy Loading
                logging.info("üìú Scrolling to load dynamic content...")
                for scroll_pct in [0.25, 0.5, 0.75, 1.0]:
                    self.driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {scroll_pct});")
                    time.sleep(2)
                
                self.driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(2)
                
                page_source = self.driver.page_source # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ HTML ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
                
                if len(page_source) > 500:
                    logging.info(f"‚úÖ Page fetched successfully ({len(page_source)} chars)")
                    return page_source
                
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Error on attempt {attempt+1}: {e}")
                
            if attempt < retries - 1:
                time.sleep(5 * (attempt + 1))
        
        logging.error("‚ùå Failed to fetch page after all retries")
        return None


    def _is_valid_thai_name(self, text: str) -> bool:
        """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        if not text or len(text) < 6 or len(text) > 90:
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 10 ‡∏Ñ‡∏≥ (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠)
        words = text.split()
        if len(words) > 10:
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏±‡∏Å‡∏£‡∏≤‡∏ä
        if re.search(r'25\d{2}', text):
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏´‡∏£‡∏∑‡∏≠ URL
        if '@' in text or 'http' in text.lower() or '.com' in text.lower() or '.th' in text.lower():
            return False
        
        # ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô
        invalid_keywords = [
            '‡∏™‡∏á‡∏ß‡∏ô‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', '‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', 'copyright', '¬©', 'all rights reserved',
            '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö‡∏°‡∏à', '‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡∏°‡∏´‡∏≤‡∏ä‡∏ô', 'limited', 'public', 'company',
            '‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå', 'website', 'www.', 'http', '.com', '.th', '.co',
            '‡πÇ‡∏ó‡∏£', '‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå', 'telephone', 'tel:', 'email', 'e-mail',
            '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠', 'contact', '‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°', 'information', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•',
            '‡πÅ‡∏ú‡∏ô‡∏Å', '‡∏ù‡πà‡∏≤‡∏¢', 'department', 'division', 'section',
            '‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà', 'address', '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà', 'location', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà',
            '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'date', '‡πÄ‡∏ß‡∏•‡∏≤', 'time', '‡∏õ‡∏µ', 'year',
            '‡∏û.‡∏®.', '‡∏Ñ.‡∏®.', 'a.d.', 'b.e.',
            '‡πÄ‡∏°‡∏ô‡∏π', 'menu', '‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å', 'home', '‡∏Å‡∏•‡∏±‡∏ö', 'back',
            '‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤', 'search', '‡∏†‡∏≤‡∏©‡∏≤', 'language', '‡πÑ‡∏ó‡∏¢', 'english',
            '‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î', 'download', 'pdf', 'print', '‡∏û‡∏¥‡∏°‡∏û‡πå',
            '‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°', 'more', '‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°', 'read more',
            '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®', 'announcement', '‡∏Ç‡πà‡∏≤‡∏ß', 'news',
            '‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢', 'policy', '‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç', 'terms', 'conditions',
            '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß', 'privacy', '‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ', 'cookie',
        ]
        
        text_lower = text.lower()
        for keyword in invalid_keywords:
            if keyword.lower() in text_lower:
                return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©
        special_char_count = sum(1 for char in text if char in '¬©¬Æ‚Ñ¢@#$%^&*()_+=[]{}|\\:;"<>,.?/')
        if special_char_count > 2:
            return False
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏ï‡∏±‡∏ß
        thai_char_count = sum(1 for char in text if 0x0E00 <= ord(char) <= 0x0E7F)
        if thai_char_count < 5:
            return False
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        thai_titles = ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß', '‡∏î‡∏£.', '‡∏î‡∏£', '‡∏®.', '‡∏£‡∏®.', '‡∏ú‡∏®.', '‡∏û‡∏±‡∏ô‡∏ï‡∏£‡∏µ', '‡∏û‡∏•.', '‡∏û‡∏•‡πÄ‡∏≠‡∏Å', '‡∏û‡∏•‡πÇ‡∏ó', '‡∏û‡∏•‡∏ï‡∏£‡∏µ']
        if not any(text.startswith(title) for title in thai_titles):
            if not any(title in text for title in thai_titles):
                return False
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏Ñ‡∏≥ (‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ + ‡∏ä‡∏∑‡πà‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ä‡∏∑‡πà‡∏≠ + ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•)
        if len(words) < 2:
            return False
        
        return True


    def _is_valid_position(self, text: str) -> bool:
        """Check if text is a valid position title"""
        if not text or len(text) < 2:
            return False
        
        valid_keywords = [
            '‡∏ú‡∏π‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£', '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£', '‡∏ú‡∏π‡πâ‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Å‡∏≤‡∏£', 
            '‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ò‡∏≤‡∏ô', '‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢', '‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤', '‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö',
            'CEO', 'CFO', 'CTO', 'COO', 'President', 'Vice',
            'Executive', 'Director', 'Manager', 'Chief', 'Officer',
            'Assistant', 'Deputy', 'Senior', 'Head', 'Business',
            '‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤', '‡πÄ‡∏•‡∏Ç‡∏≤‡∏ô‡∏∏‡∏Å‡∏≤‡∏£', '‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó',
            '‡∏Å‡∏•‡∏∏‡πà‡∏°', '‡∏ù‡πà‡∏≤‡∏¢', '‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', '‡πÅ‡∏ú‡∏ô‡∏Å', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å', '‡∏Å‡∏£‡∏°', '‡∏Å‡∏≠‡∏á',
            'Account', 'Advisor', 'Analyst', 'Audit', 'Bank', 'Board',
            'Business', 'Commercial', 'Company', 'Compliance', 'Control',
            'Corporate', 'Credit', 'Customer', 'Development', 'Division',
            'Finance', 'Financial', 'Group', 'Investment', 'Legal',
            'Marketing', 'Operation', 'Product', 'Relationship', 'Risk',
            'Sales', 'Service', 'Strategy', 'Technology', 'Treasury',
            'Unit', 'Wealth'
        ]
        
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in valid_keywords)


    def extract_executives_from_html(self, html_content: str) -> List[Tuple[str, str]]:
        """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ HTML"""
        soup = BeautifulSoup(html_content, 'html.parser')
        executives = []
        
        logging.info("\nüîç Extracting executives from HTML...")
        
        # Save HTML for debugging
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        logging.info("üíæ Saved page content to debug_page.html for inspection")

        # ‡∏•‡∏ö Element ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô
        for script in soup(["script", "style", "noscript"]):
            script.decompose()
        
        # METHOD 1.1: Check tables first: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Element <table> 
        # ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô _is_valid_thai_name ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô Cell 
        # ‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô _is_valid_position
        
        tables = soup.find_all('table')
        logging.info(f"üìä Found {len(tables)} tables")
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö
        for table_idx, table in enumerate(tables):
            rows = table.find_all('tr') # ‡∏î‡∏∂‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á

            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    # Try to find name and position pairs
                    cell_texts = [cell.get_text(strip=True) for cell in cells]
                    # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                    cell_texts = [re.sub(r'\s+', ' ', text) for text in cell_texts if text]
                    
                    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Cell ‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß
                    for i in range(len(cell_texts)):
                        text = cell_texts[i]
                        if self._is_valid_thai_name(text):
                            name = text
                            position = "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
                            
                            # Look for position in adjacent cells
                            for j in range(len(cell_texts)):
                                # ‡∏Ç‡πâ‡∏≤‡∏° Cell ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                                if i != j and self._is_valid_position(cell_texts[j]):
                                    position = cell_texts[j]
                                    break
                            
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡πÉ‡∏ô executives ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥)
                            if not any(name == existing_name for existing_name, _ in executives):
                                executives.append((name, position))
                                logging.info(f"‚úÖ Table: {name} | {position}")

        # METHOD 1.2: Check div/section containers: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏´‡∏≤ Element Container
        # (div/section/article) ‡∏ó‡∏µ‡πà‡∏°‡∏µ Class Name ‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£
        # (‡πÄ‡∏ä‡πà‡∏ô executive, management, board) ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
        containers = soup.find_all(['div', 'section', 'article'], 
                                   class_=re.compile(r'(executive|management|board|director|team|profile|member|card)', re.I))
        
        logging.info(f"üì¶ Found {len(containers)} potential executive containers")
        
        for container in containers:
            # Extract text from immediate children only (not nested deeply)
            texts = []
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Element ‡∏•‡∏π‡∏Å‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Ç‡∏≠‡∏á Container
            for child in container.find_all(recursive=False):
                child_text = child.get_text(strip=True)
                if child_text and len(child_text) > 5:
                    texts.append(re.sub(r'\s+', ' ', child_text))
            
            # Also check direct text content
            for element in container.find_all(['p', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div']):
                text = element.get_text(strip=True)
                if text and len(text) > 5 and len(text) < 150:  # Limit length
                    text = re.sub(r'\s+', ' ', text)
                    if text not in texts:
                        texts.append(text)
            
            # Process texts
            i = 0
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
            while i < len(texts):
                text = texts[i]
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                if self._is_valid_thai_name(text):
                    name = text
                    position = "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                    for j in range(i+1, min(i+4, len(texts))):
                        # ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô ‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô position
                        if self._is_valid_position(texts[j]) and not self._is_valid_thai_name(texts[j]):
                            position = texts[j]
                            break
                        
            
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡πÉ‡∏ô executives ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥)
                    if not any(name == existing_name for existing_name, _ in executives):
                        executives.append((name, position)) # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏•‡∏á‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£
                        logging.info(f"‚úÖ Container: {name} | {position}") 
                    
                    # ‡∏Ç‡∏¢‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    i += 1
                else: # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏Ç‡∏¢‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    i += 1
        
        # METHOD 1.3: List items: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Element <li> ‡∏´‡∏£‡∏∑‡∏≠ <dt> 
        # (‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ List) ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        list_items = soup.find_all(['li', 'dt'])
        logging.info(f"üìã Found {len(list_items)} list items")
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô List
        for item in list_items:
            item_text = item.get_text(strip=True) # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
            item_text = re.sub(r'\s+', ' ', item_text) # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô
            
            # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà (\n ‡∏´‡∏£‡∏∑‡∏≠ \r) 
            # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Element ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÅ‡∏ï‡πà‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
            parts = re.split(r'[\n\r]+', item_text)
            parts = [p.strip() for p in parts if p.strip()]

            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            for part in parts:
                if self._is_valid_thai_name(part): # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    name = part
                    position = "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
                    
                    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                    for other_part in parts:
                        # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                        if other_part != name and self._is_valid_position(other_part):
                            position = other_part # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
                            break

                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡πÉ‡∏ô executives ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥)
                    if not any(name == existing_name for existing_name, _ in executives):
                        executives.append((name, position))
                        logging.info(f"‚úÖ List: {name} | {position}")
        
        logging.info(f"\nüìä Total executives found: {len(executives)}")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• 10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å for debugging
        if executives:
            logging.info("\nüîç First 10 executives found:")
            for i, (name, position) in enumerate(executives[:10]):
                logging.info(f"  {i+1}. {name} - {position}")
        
        return executives

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏¢‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠
    def _parse_name_components(self, full_name: str) -> Tuple[str, str, str, str]:
        """Parse name into prefix, first name, and surname"""
        title_map = {
            "‡∏ô‡∏≤‡∏¢": "Mr.",
            "‡∏ô‡∏≤‡∏á": "Mrs.",
            "‡∏ô‡∏≤‡∏á‡∏™‡∏≤‡∏ß": "Ms.",
            "‡∏î‡∏£.": "Dr.",
            "‡∏î‡∏£": "Dr.",
            "‡∏®.": "Prof.",
            "‡∏£‡∏®.": "Assoc. Prof.",
            "‡∏ú‡∏®.": "Asst. Prof.",
            "‡∏û‡∏±‡∏ô‡∏ï‡∏£‡∏µ": "Lt."
        }
        
        prefix = ""
        name_without_prefix = full_name
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°
        for thai_title, eng_title in sorted(title_map.items(), key=lambda x: len(x[0]), reverse=True):
            if full_name.startswith(thai_title):
                prefix = eng_title
                name_without_prefix = full_name[len(thai_title):].strip()
                break
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ï‡πâ‡∏ô‡∏ä‡∏∑‡πà‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô "‡∏™‡∏°‡∏ä‡∏≤‡∏¢ ‡∏ô‡∏≤‡∏¢ ‡πÉ‡∏à‡∏î‡∏µ")
        if not prefix and any(0x0E00 <= ord(char) <= 0x0E7F for char in full_name):
            for thai_title, eng_title in title_map.items():
                if thai_title in full_name and full_name.index(thai_title) < 10:
                    prefix = eng_title
                    name_without_prefix = full_name.replace(thai_title, '').strip()
                    break
        
        name_without_prefix = re.sub(r'\s+', ' ', name_without_prefix).strip()
        
        # ‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á (parts[0]) ‡πÅ‡∏•‡∏∞‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• (parts[1:])
        parts = name_without_prefix.split()
        
        if len(parts) == 0:
            return prefix, full_name, "", ""
        elif len(parts) == 1:
            return prefix, full_name, parts[0], ""
        elif len(parts) == 2:
            return prefix, full_name, parts[0], parts[1]
        else:
            first_name = parts[0]
            surname = " ".join(parts[1:])
            return prefix, full_name, first_name, surname


    def create_executive_records(self, executives: List[Tuple[str, str]]) -> List[Dict]:
        """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á List ‡∏Ç‡∏≠‡∏á Dictionary (Records) ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Tuple ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Scrape"""
        records = []
        seen_names = set()
        
        logging.info("\nüîç Creating executive records...")
        
        #‡∏Å‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥
        for name, position in executives:
            if name in seen_names:
                logging.debug(f"  ‚ö†Ô∏è Skipping duplicate: {name}")
                continue
            
            # Additional filtering for non-name texts
            name_lower = name.lower()
            skip_keywords = ['‡∏™‡∏á‡∏ß‡∏ô‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', '‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå', '‡∏ö‡∏°‡∏à', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', 'copyright', '¬©']
            if any(keyword in name_lower for keyword in skip_keywords):
                logging.debug(f"  ‚ö†Ô∏è Skipping non-name text: {name}")
                continue
            
            # Check for year
            if re.search(r'25\d{2}', name):
                logging.debug(f"  ‚ö†Ô∏è Skipping text with year: {name}")
                continue
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏¢‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠
            prefix, full_name, first_name, surname = self._parse_name_components(name)
            
            if not first_name:
                logging.warning(f"  ‚ö†Ô∏è Could not parse name: {name}")
                continue
            
            # Validate name length
            if len(first_name) < 2 or len(first_name) > 50:
                logging.debug(f"  ‚ö†Ô∏è Invalid name length: {first_name}")
                continue
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dictionary Record ‡∏ó‡∏µ‡πà‡∏°‡∏µ Field ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ 7 Field
            record = {
                "BUSI_DT": self.busi_dt,
                "Prefixed_Name": prefix,
                "Full_Name": full_name,
                "First_Name": first_name,
                "Surname": surname,
                "Bank_Name": self.bank_name,
                "Position": position
            }
            
            records.append(record)
            seen_names.add(name)
            logging.info(f"  ‚úÖ {prefix} | {first_name} {surname} | {position}")
        
        if records:
            logging.info("\nüîç First 3 records structure:")
            for i, record in enumerate(records[:3]):
                logging.info(f"  {i+1}. Prefixed: '{record['Prefixed_Name']}' | First: '{record['First_Name']}' | Last: '{record['Surname']}'")
        
        return records


    # Main scraping function
    def intelligent_scrape(self, limit: int = 150) -> List[Dict]:
        logging.info("üöÄ Starting scraping process...")
        
        # Fetch page
        html_content = self.fetch_page_content(self.base_url)
        if not html_content:
            logging.error("‚ùå Failed to fetch page content")
            return []
        
        # Detect bank
        self.bank_name = self.detect_bank_name(self.base_url, html_content)
        logging.info(f"üè¶ Bank: {self.bank_name}")
        logging.info(f"üìÖ Business Date: {self.busi_dt}")
        
        # Extract executives
        executives = self.extract_executives_from_html(html_content)
        
        if not executives:
            logging.error("‚ùå No executives found")
            return []
        
        # Create records
        records = self.create_executive_records(executives)
        
        logging.info(f"\nüìä Total records created: {len(records)}")
        return records[:limit] # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô Records ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Record ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 150)

    def close(self):
        """Close WebDriver"""
        try:
            if self.driver:
                self.driver.quit()
                logging.info("‚úÖ WebDriver closed")
        except Exception as e:
            logging.error(f"‚ö†Ô∏è Error closing WebDriver: {e}")



def save_to_csv(data: List[Dict], bank_name: str, busi_dt: str) -> bool:
    """Save data to CSV with proper formatting"""
    if not data:
        logging.warning("‚ö†Ô∏è No data to save")
        return False

    try:
        df = pd.DataFrame(data) # Create Pandas DataFrame from list of dictionaries
        

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Column ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        column_order = ['BUSI_DT', 'Prefixed_Name', 'Full_Name', 
                       'First_Name', 'Surname', 'Bank_Name', 'Position']
        
        for col in column_order:
            if col not in df.columns:
                df[col] = ""
        
        df = df[column_order]
        
        # Clean bank name for filename
        bank_short = bank_name.replace('‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£', '').strip()
        
        """
        ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (‡πÄ‡∏ä‡πà‡∏ô '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢' ‡πÄ‡∏õ‡πá‡∏ô 'Kbank') 
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå CSV (‡πÄ‡∏ä‡πà‡∏ô Kbank_20251028.csv)
        """

        bank_name_map = {
            '‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢': 'Kbank',
            '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': 'Bangkok',
            '‡πÑ‡∏ó‡∏¢‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå': 'SCB',
            '‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢': 'Krungthai',
            '‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤': 'Krungsri',
            '‡∏≠‡∏≠‡∏°‡∏™‡∏¥‡∏ô': 'GSB',
            '‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï': 'TTB',
            '‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏≤‡∏Ñ‡∏¥‡∏ô‡∏†‡∏±‡∏ó‡∏£': 'KKP',
            '‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï': 'Thanachart',
            '‡∏ó‡∏¥‡∏™‡πÇ‡∏Å‡πâ': 'TISCO',
        }
        
        file_bank_name = bank_short
        for thai_name, eng_name in bank_name_map.items():
            if thai_name in bank_short:
                file_bank_name = eng_name
                break
        
        date_str = busi_dt.replace('-', '')
        filename = f"{file_bank_name}_{date_str}.csv"
        
        # Create output directory
        os.makedirs('output', exist_ok=True)
        output_path = os.path.join('output', filename)
        
        # Save CSV with proper encoding
        df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)
        
        logging.info(f"\n‚úÖ File saved: {output_path}")
        logging.info(f"üìä Total records: {len(df)}")
        
        print("\n" + "="*120)
        print(f"üìä RESULTS FOR {bank_name}")
        print(f"üìÖ Date: {busi_dt}")
        print(f"üìÅ File: {output_path}")
        print(f"üìà Records: {len(df)}")
        print("="*120)
        
        print("\nüìã SAMPLE DATA (first 5 records):")
        sample_df = df.head().copy()
        pd.set_option('display.unicode.east_asian_width', True)
        pd.set_option('display.max_colwidth', 30)
        print(sample_df.to_string(index=False))
        
        print("="*120 + "\n")
        
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error saving CSV: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Main execution"""
    print("="*120)
    print("ü§ñ IMPROVED BANK EXECUTIVE SCRAPER v2.0")
    print("="*120)
    print("‚úÖ Better HTML structure handling")
    print("‚úÖ Improved name/position separation")
    print("‚úÖ Prevents concatenated names issue")
    print("‚úÖ Works with various website structures")
    print("="*120 + "\n")
    
    # List of URLs to scrape
    urls = [
        "https://www.bangkokbank.com/th-TH/About-Us/Board-Directors"
        ]
    
    print("üìã URLs to scrape:")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    print()
    
    all_results = []
    
    """ 
    ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Class FlexibleBankScraper ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô 
    intelligent_scrape() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ URL
    """
    for url in urls:
        print(f"\n{'='*120}")
        print(f"üåê Processing: {url}")
        print(f"{'='*120}\n")
        
        scraper = None
        
        try:
            scraper = FlexibleBankScraper(url)
            
            print(f"üåê Target URL: {url}")
            print(f"üìÖ Date: {scraper.busi_dt}")
            
            temp_bank = scraper.detect_bank_name(url, None)
            print(f"üè¶ Initial bank detection: {temp_bank}\n")
            
            # Run scraping
            executives = scraper.intelligent_scrape()
            
            if executives:
                print(f"\nüè¶ Final detected bank: {scraper.bank_name}")
                
                # Save to CSV
                if save_to_csv(executives, scraper.bank_name, scraper.busi_dt):
                    print(f"\n‚úÖ SUCCESS: Extracted {len(executives)} executives from {scraper.bank_name}")
                    all_results.append({
                        'bank': scraper.bank_name,
                        'count': len(executives),
                        'url': url
                    })
                else:
                    print(f"\n‚ö†Ô∏è WARNING: Data extracted but failed to save CSV")
            else:
                print(f"\n‚ùå FAILED: No executives found for {url}")
            
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è Interrupted by user")
            break
        except Exception as e:
            logging.error(f"‚ùå Error processing {url}: {e}")
            import traceback
            traceback.print_exc()
        finally: # Ensure WebDriver is closed
            if scraper:
                try:
                    scraper.close()
                except:
                    pass
    
    # Summary
    print("\n" + "="*120)
    print("üìã SCRAPING SUMMARY")
    print("="*120)
    
    if all_results:
        print(f"\n‚úÖ Successfully scraped {len(all_results)} bank(s):\n")
        for i, result in enumerate(all_results, 1):
            print(f"  {i}. {result['bank']}: {result['count']} executives")
            print(f"     URL: {result['url']}\n")
    else:
        print("\n‚ùå No banks were successfully scraped")
    
    print("="*120)
    print("\nüí° TIP: Check the 'output' folder for generated CSV files")
    print("üí° TIP: Check 'debug_page.html' if you need to inspect the page structure")
    print("üí° TIP: The improved version prevents name concatenation issues")
    print("="*120 + "\n")


if __name__ == "__main__":
    main()